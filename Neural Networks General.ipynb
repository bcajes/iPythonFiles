{
 "metadata": {
  "name": "",
  "signature": "sha256:c5f8c9d98965843405a477b2598a0a39f81c46290a16909a5290259877f3271d"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Neural Networks Math Extended**: [Playlist](https://www.youtube.com/playlist?list=PL513vUSBNcuEwRoYYRG9zUuwFoT1avgjG)\n",
      "\n",
      "1. Feedforward Neural Network\n",
      "    1. artificial neuron\n",
      "    2. activation function\n",
      "    3. capacity of a single neuron\n",
      "    4. multilayer neural network\n",
      "    5. capacity of a neural network\n",
      "    6. biological inspiration\n",
      "2. Training Neural Networks\n",
      "    1. empirical risk minimization\n",
      "    2. loss function\n",
      "    3. output layer gradient\n",
      "    4. hidden layer gradient\n",
      "    5. activation function derivative\n",
      "    6. paramter gradient\n",
      "    7. backpropagation\n",
      "    8. regularization\n",
      "    9. parameter initialization\n",
      "    10. model selection\n",
      "    11. optimization\n",
      "3. CRFs - Conditional Random Fields\n",
      "    1. motivation\n",
      "    2. linear chain CRF\n",
      "    3. context window\n",
      "    4. computing the partition function\n",
      "    5. computing marginals\n",
      "    6. performing classification\n",
      "    7. factors, sufficient statistics and linear CRF\n",
      "    8. Markov network\n",
      "    9. factor graph\n",
      "    10. belief propagation\n",
      "4. Training CRFs\n",
      "    1. loss function\n",
      "    2. unary log-factor gradient\n",
      "    3. pairwise log-factor gradient\n",
      "    4. discriminitive v. generative learning\n",
      "    5. maximum-entropy Markov model\n",
      "    6. hidden Markov model\n",
      "    7. general conditional random field\n",
      "    8. pseudo likelihood\n",
      "5. Restricted Boltzmann machine\n",
      "    1. definition\n",
      "    2. inference\n",
      "    3. free energy\n",
      "    4. contrastive divergence\n",
      "    5. contrastive divergence (parameter update)\n",
      "    6. persistent CD\n",
      "    7. example\n",
      "    8. extensions\n",
      "6. Autoencoder\n",
      "    1. definition\n",
      "    2. loss function\n",
      "    3. example\n",
      "    4. linear autoencoder\n",
      "    5. undercomplete v. overcomplete hidden layer\n",
      "    6. denoising autoencoder\n",
      "    7. contractive autoencoder\n",
      "7. Deep learning\n",
      "    1. motivation\n",
      "    2. difficulty of training\n",
      "    3. unsupervised pre-training\n",
      "    4. example\n",
      "    5. dropout\n",
      "    6. deep autoencoder\n",
      "    7. deep belief network\n",
      "    8. variational bound\n",
      "    9. DBN pre-training\n",
      "8. Sparse coding\n",
      "    1. definition\n",
      "    2. inference (ISTA algorithm)\n",
      "    3. dictionary update with projected gradient descent\n",
      "    4. dictionary update with block-coordinate descent\n",
      "    5. dictionary learning algorithm\n",
      "    6. online dictionary learning algorithm\n",
      "    7. ZCA preprocessing\n",
      "    8. feature extraction\n",
      "    9. relationship with V1\n",
      "9. Computer vision\n",
      "    1. motivation\n",
      "    2. local connectivity\n",
      "    3. parameter sharing\n",
      "    4. discrete convolution\n",
      "    5. pooling and subsampling\n",
      "    6. convolutional network\n",
      "    7. object recognition\n",
      "    8. example\n",
      "    9. data set expansion\n",
      "10. Natural Language Processing\n",
      "    1. motivation\n",
      "    2. preprocessing\n",
      "    3. on-hot encoding\n",
      "    4. word representations\n",
      "    5. language modeling\n",
      "    6. neural network language model\n",
      "    7. hierarchical output layer\n",
      "    8. word tagging\n",
      "    9. convolutional network\n",
      "    10. multitask learning\n",
      "    11. recursive network\n",
      "    12. merging representations\n",
      "    13. tree inference\n",
      "    14. recursive network training\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- **Neural Networks** [Page](http://hebb.mit.edu/courses/9.641/lectures/index.html)\n",
      "    1. Brief introduction to neurons\n",
      "    2. From spikes to rates, Network organization\n",
      "    3. Perceptrons: simple and multilayer\n",
      "    4. Perceptrons as models of vision\n",
      "    5. Learning the average. Stochastic gradient learning. Competitive learning.\n",
      "    6. Learning from variance. Principal component analysis\n",
      "    7. Perceptrons as models of visual development\n",
      "    8. Delta rule, Pavlovian conditioning\n",
      "    9. Backpropagation learning\n",
      "    10. Convolutional networks. Backprop\n",
      "    11. Subsampling\n",
      "    12. Memorization and generalization\n",
      "    13.  Convolutional networks for object recognition\n",
      "        - D. Ciresan, U. Meier, L. M. Gambardella, and J. Schmidhuber. Deep, Big, Simple Neural Nets for Handwritten Digit Recognition. Neural Comput. 22, 3207\u00e2\u20ac\u201c20 (2010).\n",
      "        - D. Ciresan, U. Meier, J. Masci, L. M. Gambardella, and J. Schmidhuber. Flexible, High Performance Convolutional Neural Networks for Image Classification. Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence, 1237-42 (2011).\n",
      "        - A. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet Classification with Deep Convolutional Neural Networks. Adv. Neural Info. Proc. Syst. 25, 1106-14 (2012).\n",
      "    14.  Convolutional networks for boundary detection\n",
      "        - D. R. Martin, C. C. Fowlkes, and J. Malik. Learning to Detect Natural Image Boundaries Using Local Brightness, Color, and Texture Cues. IEEE Trans. Patt. Anal. Mach. Intell. 26, 530-49 (2004). This paper introduced the Berkeley Segmentation Dataset and Benchmark.\n",
      "        - S. C. Turaga, J. F. Murray, V. Jain, F. Roth, M. Helmstaedter, K. Briggman, W. Denk, and H. S. Seung. Convolutional Networks Can Learn to Generate Affinity Graphs for Image Segmentation. Neural Comput. 22, 511-38 (2010).\n",
      "        - D. C. Ciresan, A. Giusti, L. M. Gambardella, and J. Schmidhuber. Deep Neural Networks Segment Neuronal Membranes in Electron Microscopy Images. Adv. Neural Info. Proc. Syst. 25, 2852-60 (2012). This was the winning entry in the ISBI 2012 Challenge: Segmentation of neuronal structures in EM stacks.\n",
      "        - ISBI 2013 challenge: 3D segmentation of neurites in EM images\n",
      "    15. Convolutional networks for image restoration\n",
      "        - V. Jain and H. S. Seung. Natural Image Denoising with Convolutional Networks. Adv. Neural Info. Proc. Syst. 21, 769-76 (2008).\n",
      "        - J. Xie, L. Xu, and E. Chen. Image Denoising and Inpainting with Deep Neural Networks. Adv. Neural Info. Proc. Syst. 25, 350-8 (2012)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}