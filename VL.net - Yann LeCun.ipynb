{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Unreasonable Effectiveness of Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yann LeCun (http://yann.lecun.com)(http://videolectures.net/yann_lecun/)     - Facebook AI Research & Center for Data Science, NYU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VideoLectures.net location (http://videolectures.net/sahd2014_lecun_deep_learning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 55 Years of hand-crafted features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Traditional model of pattern recognition (since the late 50's)\n",
    "    - Fixed/engineered features (or fixed kernel) + trainable classifier\n",
    "        - Image -> hand-crafted Feature Extractor -> \"Simple\" Trainable Classifier\n",
    "        - Build feature extractor -> Classifier -> train classifier\n",
    "- Perceptron\n",
    "                                                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Architecture of \"Classical\" Recognition Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \"Classic\" architecture for pattern recognition\n",
    "    - Speech, and Object recognition (until recently)\n",
    "    - Handwriting recognition (long ago)\n",
    "    - Graphical model has latent variables (location of parts)\n",
    "    - Fixed front end feature extractor -> separate separate levels of features -> Classifier -> train classifier\n",
    "    - The way of doing machine learning until about 2 years ago"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Architecture of Deep Learning-Based Recognition Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \"Deep\" architecture for pattern recognition\n",
    "    - Speech, and Object recognition (recently)\n",
    "    - Handwriting recognition (since the 1990s)\n",
    "    - Convolutional Net with optional Graphical model on top\n",
    "    - Trained purely supervised\n",
    "    - Graphical model has latent variables (locatin of parts)\n",
    "    - stacking a bunch of parameterized function in a supervised way\n",
    "    - post-processing reasoning that would require some sort of optimization\n",
    "    - conditional random field (http://en.wikipedia.org/wiki/Conditional_random_field)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Future Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Globally-trained *deep architecture*\n",
    "    - Handwriting recognition (since the mid 1990s)\n",
    "    - All the modules are trained with a combination of unsupervised and supervised learning\n",
    "    - End-to-end training == deep structured prediction\n",
    "    - we would eventually integrated supervised and unsupervised learning together (where we have a ton of data but not a lot of labeled data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Learning = Learning Hierarchical Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we need a working learning rule that can work/scale (boltzmann's don't work and require sampling)\n",
    "- hierarchical architecture - the world is compositional\n",
    "- notable local features are extracted (with high correlations between adjacent patches of image) \n",
    "- compositions of lower level features -> eliminate redundant variability in a signal by increasing the number of representations or decreasing the resolution of features in the next level\n",
    "- It's *deep* if it has *more than one stage* of non-linear feature transformation\n",
    "- Feature visualization of convolutional net trained on ImageNet:\n",
    "    - Zieler & Fergus 2013, *Visualizing and Understanding Convolutional Networks*, (http://www.cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf)\n",
    "    - Matthew Zeiler (https://scholar.google.com/citations?user=a2KklUoAAAAJ&hl=en&oi=sra)\n",
    "    - Rob Fergus (https://scholar.google.com/citations?user=GgQ9GEkAAAAJ&hl=en)\n",
    "    - convolutional neural networks (http://deeplearning.net/tutorial/lenet.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trainable Feature Hierarchy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hierarchy of representations with increasing level of abstraction in all kinds of signals\n",
    "- Each stage is a kind of trainable feature transform\n",
    "- Image recognition:\n",
    "    - Pixel -> edge -> texton -> motif -> part -> object\n",
    "- Text:\n",
    "    - character -> word -> word group -> clause -> sentence -> story\n",
    "- Speech:\n",
    "    - sample -> spectral band -> sound -> ... -> phone -> phoneme -> word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Representations: a Challenge for ML, CV, AI, Neuroscience, Cognitive Science ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data is made of high-dimensional hierarchical relationships - how do we learn relationships to represent them by just observing it?\n",
    "- How do we learn representations of the perceptual world?\n",
    "    - How can a perceptual system build itself by looking at the world?\n",
    "    - How much prior is necessary?\n",
    "- ML/AI: how do we learn features or feature hierarchies?\n",
    "    - What is the fundamental principle? What is the learning algorithm? What is the architecture?\n",
    "- Neuroscience: how does the cortex learn perception?\n",
    "    - Does the cortex \"run\" a single, general learning algorithm? (or a small number of them)\n",
    "- CogSci: how does the mind learn abstract concepts on top of less abstract ones?\n",
    "- Deep Learning addresses the problem of learning hierarchical representations with a single algorithm\n",
    "    - or perhaps with a few algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Mammalian Visual Cortex is Hierarchical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The ventral (recognition) pathway in the visual cortex has multiple stages \n",
    "- Retina - LGN - V1 - V2 - V4 - PIT - AIT ...\n",
    "- Lots of intermediate representations\n",
    "- (FIGURE) Van Essen & Gallant, 1994, *Neural Mechanisms of Form and Motion Processing in the Primate Visual System*, (http://cognitrn.psych.indiana.edu/busey/temp/illusoryconju%20refs/vanEssen_gallant94.pdf)\n",
    "    - DC Van Essen (https://scholar.google.com/scholar?q=DC+Van+Essen&btnG=&hl=en&as_sdt=0%2C10)\n",
    "    - JL Gallant (https://scholar.google.com/scholar?q=JL+Gallant&btnG=&hl=en&as_sdt=0%2C10)\n",
    "- (FIGURE) S Thorpe, D Fize, C Marlot, 1996, *Speed of processing in the human visual system*, (http://vpl.uchicago.edu/pages/courses/sp2005/Thorpe96.pdf)\n",
    "    - Simon Thorpe (https://scholar.google.com/citations?user=uR-7ex4AAAAJ&hl=en&oi=ao)\n",
    "- \"there is this sense that for very fast perception it the brain uses mostly feedforward pathways with relatively little feedback, since it is just too fast.\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which Models are Deep?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2-layer models are not deep (even if you train the first layer)\n",
    "    - BECAUSE THERE IS NO FEATURE HIERARCHY\n",
    "- Neural nets with 1 hidden layer are not deep\n",
    "- SVMs and Kernel methods are not deep\n",
    "    - Layer1: kernels; Layer2: linear\n",
    "    - The first layer is \"trained\" in with the simplest unsupervised method ever devised: using the samples as templates for the kernel functions\n",
    "    - \"*glorified template matching*\"\n",
    "- Classification trees are not deep\n",
    "    - No hierarchy of features. ALl decisions are made in input space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are Good Features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discovering the Hidden Structure in High-Dimensional Data: The Manifold Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Learning Representations of Data:\n",
    "    - Discovering and disentangling the independent explanaotry factors\n",
    "- The Manifold Hypothesis:\n",
    "    - Natural data lives in low-dimensional (non-linear) manifold\n",
    "    - Because variables in natural data\n",
    "- Example: all face images of a person\n",
    "    - 1000x1000 pixels = 1,000,000 dimensions\n",
    "    - But the face has 3 cartesian coordinates and 3 Euler angles\n",
    "    - And humans have less than about 50 muscles in the face\n",
    "    - Hence the manifold of face images for a person has <56 dimensions\n",
    "- The perfect representations of a face image:\n",
    "    - The coordinates on the face manifold\n",
    "    - The coordinates away from the manifold\n",
    "- We do not have good and general methods to learn functions that turns an image into this kind of representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Idea for Invariant Feature Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Embed the input *non-linearly* into a high(er) dimensional space\n",
    "    - In the new space, things that were non separable may become separable\n",
    "- Pool regions of the new space together\n",
    "    - Bringing togehter things that are semantically similar. Like pooling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sparse Non-Linear Expansion -> Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overall Architecture: multiple stages of Normalization -> Filter Bank -> Nonlinearity -> Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Normalization: variation on whitening (optional)\n",
    "    - Subtractive: average removal, high pass filtering\n",
    "    - Divisive: local contrast normalization, variance normalization\n",
    "- Filter Bank: dimension expansion, projection on overcomplete basis\n",
    "- Non-Linearity: sparsification, saturation, lateral inhibition\n",
    "    - Rectification (ReLU), Component-wise shrinking, tanh\n",
    "    $$ReLU(x)=max (x,0 )$$\n",
    "- Pooling: aggregation over space or feature type\n",
    "    $$MAX: Max_i\\Big(X_i\\Big); L_p:\\sqrt[p]{X_{i}^{p}}; PROB: \\frac{1}{2} \\log \\Big(\\sum\\mathrm{e}^{bX_i}\\Big)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Nets with ReLUs and Max Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Stack of Linear transforms interspersed with Max operators\n",
    "- Point-wise ReLUs: $$ReLU(x)=max (x,0 )$$\n",
    "- Max Pooling\n",
    "    - \"switches\" from one layer to the next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supervised Training: Stochastic (Sub)Gradient Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute all the derivatives, we use a backward sweep called the back-propagation algorithm that uses the recurrent equation for $\\frac{\\partial E}{\\partial X_i}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\\frac{\\partial E}{\\partial X_i} = \\frac{\\partial C (X_n,Y)}{\\partial (X_n)}$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math at 0x1049459d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$$\\frac{\\partial E}{\\partial X_{n-1}} = \\frac{\\partial E}{\\partial X_n}\\frac{\\partial F_n (X_{n-1},W_n)}{\\partial (X_{n-1})}$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math at 0x104945cd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$$\\frac{\\partial E}{\\partial W_{n}} = \\frac{\\partial E}{\\partial X_n}\\frac{\\partial F_n (X_{n-1},W_n)}{\\partial (W_{n})}$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math at 0x104945fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$$\\frac{\\partial E}{\\partial X_{n-2}} = \\frac{\\partial E}{\\partial X_{n-1}}\\frac{\\partial F_{n-1} (X_{n-2},W_{n-1})}{\\partial (X_{n-2})}$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math at 0x1049458d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$$\\frac{\\partial E}{\\partial X_{n-1}} = \\frac{\\partial E}{\\partial X_{n-1}}\\frac{\\partial F_{n-1} (X_{n-2},W_{n-1})}{\\partial (W_{n-1})}$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math at 0x104957090>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Math, Latex\n",
    "display(Math(r'\\frac{\\partial E}{\\partial X_i} = \\frac{\\partial C (X_n,Y)}{\\partial (X_n)}'))\n",
    "display(Math(r'\\frac{\\partial E}{\\partial X_{n-1}} = \\frac{\\partial E}{\\partial X_n}\\frac{\\partial F_n (X_{n-1},W_n)}{\\partial (X_{n-1})}'))\n",
    "display(Math(r'\\frac{\\partial E}{\\partial W_{n}} = \\frac{\\partial E}{\\partial X_n}\\frac{\\partial F_n (X_{n-1},W_n)}{\\partial (W_{n})}'))\n",
    "display(Math(r'\\frac{\\partial E}{\\partial X_{n-2}} = \\frac{\\partial E}{\\partial X_{n-1}}\\frac{\\partial F_{n-1} (X_{n-2},W_{n-1})}{\\partial (X_{n-2})}'))\n",
    "display(Math(r'\\frac{\\partial E}{\\partial X_{n-1}} = \\frac{\\partial E}{\\partial X_{n-1}}\\frac{\\partial F_{n-1} (X_{n-2},W_{n-1})}{\\partial (W_{n-1})}'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- .... etc, until we reach the first module\n",
    "- we now have all the $\\frac{\\partial E}{\\partial W_i}$ for $ i \\in [1,n]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Function for a Simple Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1-1-1 network\n",
    "    - $ Y = W_1*W_2*X $\n",
    "- Trained to compute the identity function with quadratic loss\n",
    "    - Single sample $X=1$, $Y=1$\n",
    "    - $L(W) = (1-W_1*W_2)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Nets with ReLUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Single Output:\n",
    "     $$\\hat{Y} = \\sum\\limits_{P} \\delta_{P} (W,X) (\\prod\\limits_{(ij) \\in P}W_{ij}) X_{P_{start}}$$\n",
    "- $W_{ij}$ weight from *j* to *i*\n",
    "- P: path in network from input to output\n",
    "    - *P = (3,(14,3),(22,14),(31,22))*\n",
    "- *di*: 1 if *ReLU i* is linear, 0 if saturated.\n",
    "- $X_{P_{start}}$: input unit for path *P*\n",
    "$$\\hat{Y} = \\sum\\limits_{P} \\delta_{P} (W,X) (\\prod\\limits_{(ij) \\in P}W_{ij}) X_{P_{start}}$$\n",
    "- $ Dp(W,X)$: 1 if path *P* is \"active\", 0 if inactive\n",
    "- Input-output function is piece-wise linear\n",
    "- Polynomial in *W* with random coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Convolutional Nets (and other deep neural nets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Training samlple: $(X_i,Y_i)$, *k = 1 to K*\n",
    "- Objective Function (with magin-type loss = ReLU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$L(W) = \\sum\\limits_{k} ReLU (1 - Y^{k} \\sum\\limits_{P} \\delta_{P}(W,X^{k})(\\prod\\limits_{(ij) \\in P} W_{ij})X_{P_{start}}^{k})$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math at 0x1037c52d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$$L(W) = \\sum\\limits_{k} \\sum\\limits_{P} (X_{P_{start}}^{k} Y^{k}) \\delta_{P}(W,X^{k})(\\prod\\limits_{(ij) \\in P} W_{ij})$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math at 0x1037c5250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$$L(W) = \\sum\\limits_{P} \\big[\\sum\\limits_{k} (X_{P_{start}}^{k} Y^{k}) \\delta_{P}(W,X^{k})\\big](\\prod\\limits_{(ij) \\in P} W_{ij})$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math at 0x1037c5590>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$$L(W) = \\sum\\limits_{P} C_{P}(X, Y,W) (\\prod\\limits_{(ij) \\in P} W_{ij})$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math at 0x1037c5490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Math, Latex\n",
    "display(Math(r'L(W) = \\sum\\limits_{k} ReLU (1 - Y^{k} \\sum\\limits_{P} \\delta_{P}(W,X^{k})(\\prod\\limits_{(ij) \\in P} W_{ij})X_{P_{start}}^{k})'))\n",
    "\n",
    "display(Math(r'L(W) = \\sum\\limits_{k} \\sum\\limits_{P} (X_{P_{start}}^{k} Y^{k}) \\delta_{P}(W,X^{k})(\\prod\\limits_{(ij) \\in P} W_{ij})'))\n",
    "\n",
    "display(Math(r'L(W) = \\sum\\limits_{P} \\big[\\sum\\limits_{k} (X_{P_{start}}^{k} Y^{k}) \\delta_{P}(W,X^{k})\\big](\\prod\\limits_{(ij) \\in P} W_{ij})'))\n",
    "\n",
    "display(Math(r'L(W) = \\sum\\limits_{P} C_{P}(X, Y,W) (\\prod\\limits_{(ij) \\in P} W_{ij})'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Polynomial in *W* of degree *l* (number of adaptive layers)\n",
    "- Continuous, piece-wise polynomial with \"switched\" and partially random coefficients\n",
    "    - Coefficients are switched in and out depending on *W*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Nets with ReLUs: Objective Function is Piecewise Polynomial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If we use a hinge loss, delta now depends on label $Y_k$\n",
    "$$L(W) = \\sum\\limits_{P} C_{P}(X, Y,W) (\\prod\\limits_{(ij) \\in P} W_{ij})$$\n",
    "- Piecewise polynomial in *W* with random coefficients\n",
    "- A lot is known about the distribution of critical points of polynomials on the sphere with random (Gaussian) coefficients\n",
    "    - High-order spherical spin glasses\n",
    "    - Random matrix theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le Cun, et al. 1989, *Handwritten Zip Code Recognition with Multilayer Networks*, (http://yann.lecun.com/exdb/publis/pdf/lecun-90e.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Early Hierarchical Feature Models for Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hubel & Wiesel, 1962, *Receptive Fields, Binocular Interaction and Functional Architecture in the Cat's Visual Cortex* (http://corevision.cns.nyu.edu/~tony/vns/readings/hubel-wiesel-1962.pdf)\n",
    "    - *simple cells* detect local features\n",
    "    - *complex cells* \"pool\" the outputs of simple cells within a retinotopic neighborhood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cognitron & Neocognitron [ Fukushima 1974-1982 ] https://scholar.google.com/scholar?q=K+Fukushima&btnG=&hl=en&as_sdt=0%2C10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Convolutional Net Model (Multistage Hubel-Wiesel system)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Training is supervised\n",
    "- With stochastic gradient descent\n",
    "- LeCun et al, 1989, *Optimal Brain Damage*, http://www.cnbc.cmu.edu/~plaut/IntroPDP/papers/LeCunDenkerSolla90NIPS.pdf\n",
    "- LeCun et al, 1989, *Generalization and Network Design Strategies*, http://masters.donntu.org/2012/fknt/umiarov/library/lecun.pdf\n",
    "- LeCun et al, 1998, *Gradient-based learning applied to document recognition*, https://itb.biologie.hu-berlin.de/~zito/teaching/CNSIII-2006/proj6/proj6_2.pdf\n",
    "\n",
    "- Non-linearity: half-wave rectification (ReLU), shrinkage function, sigmoid\n",
    "- Pooling: max, average, L1, L2, log-sum-exp\n",
    "- *Training*: Supervised (1988-2006), Unsupervised + Supervised (2006-Now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brute Force Approach to Multiple Object Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea #1: Sliding Window ConvNet + Weighted FSM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \"Space Displacement Neural Net\".\n",
    "- Convolutions are appplied to a large image\n",
    "- Output and feature maps are extended/replicated accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Networks in Visual Object Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We knew ConvNet worked well with characters and small images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Traffic Sign Recognition (GTSRB)\n",
    "    - German Traffic Sign Reco Bench\n",
    "    - 99.2% accuracy (IDSIA)\n",
    "- House Number Recognition (Google)\n",
    "    - Street View House Numbers\n",
    "    - 94.3% accuracy (NYU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NORB Dataset (2004): 5 categories, multiple views and illuminations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Less than 6% error on test set with cluttered backgrounds\n",
    "- 291,600 training samples\n",
    "- 58.320 test samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mid 2000s: state of the art results on face detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (Review) Tolba et al. 2006, *Face Recognition: A Literature Review*, http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.179.2182&rep=rep1&type=pdf\n",
    "- Vaillant et al. IEE 1994, * An original approach for the localization of objects in images*, http://digital-library.theiet.org/content/journals/10.1049/ip-vis_19941301\n",
    "- Osadchy et al. 2004, *Efficient detection under varying illumination conditions and image plan rotations*, http://rita.osadchy.net/papers/cviu.pdf\n",
    "- Simultaneous face detection and pose estimation\n",
    "    - Osadchy et al. JMLR 2007, *Synergistic Face Detection and Pose Estimation with Energy-Based Models*, http://www.jmlr.org/papers/volume8/osadchy07a/osadchy07a.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual Object Recognition with Convolutional Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the mid 2000s, ConvNets were getting decent results on object recognition\n",
    "- Dataset: \"Caltech101\"\n",
    "    - 101 categories\n",
    "    - 30 training samples per category\n",
    "- But the results were slightly worse than more \"traditional\" computer vision methods, because\n",
    "    1. The datasets were too small\n",
    "    2. the computers were too slow\n",
    "- But we couldn't beat the state of the art because the datasets were too small\n",
    "- *But we learned that rectification and max pooling are useful!*\n",
    "    - Jarrett et al. ICCV 2009, *What is the Best Multi-Stage Architecture for Object Recognition?*, http://yann.lecun.com/exdb/publis/pdf/jarrett-iccv-09.pdf\n",
    "        - \"show that using non-linearities that include rectification and local contrast normalization is the single most important ingredient for good accuracy on object recognition benchmarks.\n",
    "        - Most surprisingly, we show that a two-stage system with *random filters* can yield almost 63% recognition rate on Caltech-101, provided that the proper non-lineariteis and pooling layers are used. \n",
    "        - Finally, we show that will supervised refinement, the system achieves state-of-the-art performance on NORB dataset and unsupervised pre-training followed by supervised refinement produced good accuracy on Caltech-101\"\n",
    " \n",
    "- Object Recognition: Krizhevsky, Sutskever, Hinton 2012, *ImageNet Classification with Deep Convolutional Neural Networks*, (http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf)\n",
    "    - Won the 2012 ImageNet LSVRC. 60 Million parameters, 832 MAC ops\n",
    "    - employed DROPOUT\n",
    "    - Method: large convolutional net\n",
    "        - 650K neurons, 832M synapses, 60M parameters\n",
    "        - Trained with backprop on GPU\n",
    "        - Trained \"with all the tricks Yann came up with in the last 20 years, plus dropout\n",
    "        - Rectification, contrast normalization\n",
    "    - Error rate: 15% (whenever correct class isn't in top 5)\n",
    "    - Previous state of the art: 25%\n",
    "    - A REVOLUTION IN COMPUTER VISION\n",
    "    - acquired by Google in Jan 2013\n",
    "    - Deployed in Google+ Photo Tagging in May 2013"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Then, two things happened ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The ImageNet dataset \n",
    "    - Fei-Fei et al 2012, http://vision.stanford.edu/publications.html\n",
    "    - 1.5 million training samples\n",
    "    - 1000 categories\n",
    "- Fast Graphical Processing Units (GPU)\n",
    "    - Capable of 1 trillion operations/second\n",
    "- ImageNet Large-Scale Visual Recognition Challenge http://www.image-net.org/challenges/LSVRC/2014/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NYU ConvNet Trained on ImageNet: OverFeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sermanet et al. 2014, *OverFeat: Integrated Recognition, Localization and Detection Using Convolutional Neural Networks*, http://arxiv.org/pdf/1312.6229.pdf\n",
    "- Trained on GPU using Torch7 (http://torch.ch)\n",
    "- Uses a number of new tricks\n",
    "- Classification 1000 categories:\n",
    "    - 13.8% error (top 5) with an ensemble of 7 networks\n",
    "    - 15.4% error (top 5) with a single network\n",
    "- Classification+Localization\n",
    "    - 30% error\n",
    "- Detection (200 categories)\n",
    "    - 19% correct\n",
    "- Downloadable code (running, no training)\n",
    "    - Search for \"overfeatNYU\" on Google (http://cilvr.nyu.edu/doku.php?id=code:start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####     Classification + Localization: multiscale sliding window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Apply convnet with a sliding window over the image at multiple scales\n",
    "- Important note: it's very cheap to slide a convnet over an image\n",
    "    - Just complete the convolutions over the whole image and replicate the fully-connected layers\n",
    "    - Traditional Detectors/Classifiers must be applied to evey location on a large input image, at multiple scales.\n",
    "    - Convolutional nets can be replicated over large images very cheaply\n",
    "    - Simply apply the convolutions to the entire image and spatially replicate the fully-connected layers\n",
    "- For each window, predict a class and bounding box parameters\n",
    "    - Even if the object is not completely contained in the viewing window, the Convnet can predict where it thinks the object is\n",
    "- Compute an \"average\" bounding box, weighted by scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConvNets as Generic Feature Extractors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bo, Ren, Fox, 2013, * Multipath Sparse Coding Using Hierarchical Matching Pursuit*, http://research.cs.washington.edu/istc/lfb/paper/cvpr13.pdf\n",
    "    - Liefeng Bo, http://research.cs.washington.edu/istc/lfb/, https://scholar.google.com/citations?user=FJwtMf0AAAAJ&hl=en\n",
    "    - Xiaofeng Ren, https://scholar.google.com/citations?user=1KFFbEIAAAAJ&hl=en\n",
    "    - Dieter Fox, https://scholar.google.com/citations?user=DqXsbPAAAAAJ&hl=en&oi=ao\n",
    "- Sohn, Jung, Lee, Hero ICCV 2011, *Efficient Learning of Sparse, Distributed, Convolutional Feature Representations for Object Recognition*, http://web.eecs.umich.edu/~honglak/iccv2011-sparseConvLearning.pdf\n",
    "    - Kihyuk Sohn https://scholar.google.com/citations?user=IIHEmDUAAAAJ&hl=en&oi=ao\n",
    "    - Dae Yon Jung https://scholar.google.com/citations?user=xgQd1qgAAAAJ&hl=en\n",
    "    - Honglak Lee https://scholar.google.com/citations?user=fmSHtE8AAAAJ&hl=en\n",
    "    - Alfred Hero III https://scholar.google.com/citations?user=DSiNzkIAAAAJ&hl=en\n",
    "- Razavian, Azizpour, Sullivan, Carlsson, 2014, *CNN features off-the-shelf: An astounding baseline for recognition* http://www.datascienceassn.org/sites/default/files/CNN%20Features%20off-the-shelf%20-%20an%20Astounding%20Baseline%20for%20Recognition.pdf\n",
    "    - Ali Razavian https://scholar.google.com/citations?user=E3fqfDIAAAAJ&hl=en&oi=sra\n",
    "    - Hossein Azizpour https://scholar.google.com/citations?user=t6CRgJsAAAAJ&hl=en&oi=sra\n",
    "    - Josephine Sullivan https://scholar.google.com/citations?user=REbc02cAAAAJ&hl=en&oi=sra\n",
    "    - http://www.csc.kth.se/cvap/cvg/DL/ots/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other ConvNet Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Zeiler and Fergus, 2013, *Visualizing and Understanding Convolutional Networks*, http://www.cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf\n",
    "- Donahue, Jia, Vinjals, et al. 2014, *Decaf: A deep convolutional activation feature for generic visual recognition*, http://www.eecs.berkeley.edu/~nzhang/papers/icml14_decaf.pdf\n",
    "    - Jeff Donahue https://scholar.google.com/citations?user=UfbuDH8AAAAJ&hl=en&oi=sra\n",
    "    - Yangqing Jia https://scholar.google.com/citations?user=mu5Y2rYAAAAJ&hl=en&oi=sra\n",
    "    - Oriol Vinyals https://scholar.google.com/citations?user=NkzyCvUAAAAJ&hl=en\n",
    "    - Judy Hoffman https://scholar.google.com/citations?user=3dlBGiQAAAAJ&hl=en\n",
    "    - Ning Zhnag https://scholar.google.com/citations?user=DplAah0AAAAJ&hl=en\n",
    "    - Eric Tzeng https://scholar.google.com/citations?user=nABXo3sAAAAJ&hl=en\n",
    "    - Trevor Darrell https://scholar.google.com/citations?user=bh-uRFMAAAAJ&hl=en\n",
    "- Girschick et al, 2013, *Rich feature hierarchies for accurate object detection and semantic segmentation*, http://www.cs.berkeley.edu/~rbg/papers/r-cnn-arxiv.pdf\n",
    "    - Ross Girschick https://scholar.google.com/citations?user=W8VIEZgAAAAJ&hl=en&oi=sra\n",
    "    - Jitendra Malik https://scholar.google.com/citations?user=oY9R5YQAAAAJ&hl=en\n",
    "- Oquad, et al 2013, * Learning and transferring mid-level image representations using convolutional neural networks*, http://www.di.ens.fr/willow/pdfscurrent/oquab14cvpr.pdf\n",
    "    - Maxime Oquab\n",
    "    - Leon Bottou https://scholar.google.com/citations?user=kbN88gsAAAAJ&hl=en&oi=sra\n",
    "    - Ivan Laptev https://scholar.google.com/citations?user=-9ifK0cAAAAJ&hl=en&oi=sra\n",
    "    - Josef Sivic\n",
    "- Kahn, et al 2014, *Automatic Feature Learning for Robust Shadow Detection*, http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=6909646&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D6909646\n",
    "    - Salman Khan https://scholar.google.com/citations?user=M59O9lkAAAAJ&hl=en&oi=sra\n",
    "    - Mohammed Bannamoun https://scholar.google.com/citations?user=ylX5MEAAAAAJ&hl=en&oi=sra\n",
    "    - Ferdous Sohel https://scholar.google.com/citations?user=Xj1MBQcAAAAJ&hl=en&oi=sra\n",
    "- Sander Dieleman, Kaggle Galaxy Zoo Challenge 2014, http://benanne.github.io/2014/04/05/galaxy-zoo.html\n",
    "    - https://scholar.google.com/citations?user=2ZU62T4AAAAJ&hl=en&oi=ao\n",
    "    \n",
    "- RESULTS COMPILATION: http://cs.nyu.edu/~sermanet/papers/Deep_ConvNets_for_Vision-Results.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Similarity Matching With Siamese Networks Embedding, DrLIM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DrLIM: Metric Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contrative Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Siamese Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Siamese architecture and loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face Recognition: DeepFace (Facebook AI Research)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Depth Estimation from Stereo Pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Body Pose Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pose Estimation and Attribute Recovery with ConvNets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Tasks for Which Deep Convolutional Nets are the Best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning and Convolutional Networks in Speech, Audio, and Signals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acoustic Modeling in Speech Recognition (Google)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speech Recognition with Convolutional Nets (NYU/IBM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Networks in Image Segmentation, & Scene Labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConvNets for Image Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConvNet in Connectomics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Labeling/Scene Parsing: Labeling every pixel with the object it belongs to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scene Parsing/Labeling: ConvNet Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 1: majority over super-pixel regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scene Parsing/Labeling: Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scene Parsing/Laveling: SIFT Flow dataset (33 categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal Consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NYU RGB-D Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling Videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Segmentation on RGB+D Images and Videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commercial applications of Convolutional Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Software Platform for Deep Learning: Torch7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Energy-Based Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning the Energy Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seven Strategies to Shape the Energy Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. constant volume of low energy Energy surface for PCA and K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. push down of the energy of data points, push up everywhere else"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. push down of the energy of data points, push up on chosen locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary Learning with Fast Approximate Inference: Sparse Auto-Encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Modeling: Sparse Coding + Dictionary Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning to Perform Approximate Inference: Predictive Sparse Decomposition Sparse Auto-encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse auto-encoder: Predictive Sparse Decomposition (PSD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized Encoder-Decoder Model (auto-Encoder) for Unsupervised Feature Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PSD: Basis Functions on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictive Sparse Decomposition (PSD): Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learned Features on natural patches: V1-like receptive fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning to Perform the Approximate Inference LISTA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Better Idea: Give the \"right\" structure to the encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LISTA: Train We and S matrices to give a good approximation quickly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning ISTA (LISTA) vs ISTA/FISTA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LISTA with partial mutual inhibition matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Coordinate Descent (LcoD): faster than LISTA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###     Convolutional Sparse Coding (\"deconvolutional networks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zeiler, Taylor, Fergus, CVPR 2010, *Deconvolutional Networks*, http://www.uoguelph.ca/~gwtaylor/publications/mattcvpr2010/deconvolutionalnets.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional PSD: Encoder with a soft sh() Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Sparse Auto-Encoder on Natural Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using PSD to Train a Hierarchy of Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised + Supervised for Pedestrian Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
