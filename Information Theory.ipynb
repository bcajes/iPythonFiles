{
 "metadata": {
  "name": "",
  "signature": "sha256:ea60efa77beb7ca29787c155b4ac6803e89ce40b34a1879ddd33b1adb6436a41"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Information and Entropy** (Seth Lloyd) [MIT OpenCourseware](http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-050j-information-and-entropy-spring-2008/)\n",
      "\n",
      "1. Bits and Codes\n",
      "2. Compression\n",
      "3. Noise and Errors\n",
      "4. Probability\n",
      "5. Probability 2\n",
      "6. Communications\n",
      "7. Communications 2\n",
      "8. Processes\n",
      "9. Inference\n",
      "10. Inference 2\n",
      "11. Maximum Entropy\n",
      "12. Maximum Entropy 2\n",
      "13. Physical Systems\n",
      "14. Physical Systems 2\n",
      "15. Energy\n",
      "16. Energy 2\n",
      "17. Temperature\n",
      "18. Temperature 2\n",
      "19. Quantum Information\n",
      "\n",
      "- Text: [Course Notes](http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-050j-information-and-entropy-spring-2008/syllabus/MIT6_050JS08_penfield.pdf)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- **Information Theory, Pattern Recognition and Neural Networks** [page](http://videolectures.net/course_information_theory_pattern_recognition/) [text](http://www.inference.phy.cam.ac.uk/mackay/itila/)\n",
      "    1. Introduction to Information Theory\n",
      "    2. Entropy and Data Compression I: Introduction to Compression, Information Theory and Entropy\n",
      "    3. Entropy and Data Compression II: Shannon's Source Coding Theorem and the Bent Coin Lottery\n",
      "    4. Entropy and Data Compression III: Shannon's Source Coding Theorem, Symbol Codes\n",
      "    5. Entropy and Data Compression IV: Shannon's Source Coding Theorem, Symbol Codes and Arithmetic Coding\n",
      "    6. Noisy Channel Coding I: Inference and Information Measures for Noisy Channels\n",
      "    7. Noisy Channel Coding II: The Capacity of a Noisy Channel\n",
      "    8. Noisy Channel Coding III: The Noisy-Channel Coding Theorem\n",
      "    9. A Noisy Channel Coding Gem, And an Introduction to Bayesian Inference I\n",
      "    10. An Introduction to Bayesian Inference II: Inference of Parameters and Models\n",
      "    11. Approximating Probability Distributions I: Clustering as an Example Inference Problem\n",
      "    12. Approximating Probability Distributions II: Monte Carlo Methods I: Importance Sampling, Rejection Sampling, Gibbs Sampling, Metropolis Method\n",
      "    13. Approximating Probability Distributions III: Monte Carlo Methods II: Slice Sampling, Hybrid Mone Carlo, Over-relaxation, Exact Sampling\n",
      "    14. Approximating Probability Distribution IV: Variational Methods\n",
      "    15. Data Modeling With Neural Networks I: Feedforward Networks: The Capacity of a Single Neuron, Learning as Inference\n",
      "    16. Data Modeling With Neural Networks II: Content-Addressable Memories and State-Of-The-Art Error-Correcting Codes"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}