{
 "metadata": {
  "name": "",
  "signature": "sha256:134af117f18b7ef1c52bc0d0c5bb489c459df241fb645e9ed52e1478056dc786"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Undergraduate Machine Learning (Nando de Freitas)**: [Lectures](https://www.youtube.com/playlist?list=PLE6Wd9FR--Ecf_5nCbnSQMHqORpiChfJf) [Homepage](http://www.cs.ubc.ca/~nando/) [Google Scholar](https://scholar.google.com/citations?user=nzEluBwAAAAJ) [Course Page](http://www.cs.ubc.ca/~nando/340-2012/lectures.php)\n",
      "\n",
      "1. Introduction to machine learning\n",
      "2. Introduction to machine learning 2\n",
      "3. Basic probability\n",
      "4. Introduction to probability, linear algebra and PageRank\n",
      "5. Introduction to Bayes\n",
      "6. Bayes Rule and Bayesian Networks\n",
      "7. Bayesian networks, aka probabilistic graphical models\n",
      "8. Inference in Bayesian networks and dynamic programming\n",
      "9. Hidden Markov models - HMM\n",
      "10. Expectation, probability and Bernoulli models\n",
      "11. Maximum likelihood\n",
      "12. Bayesian learning\n",
      "13. Learning Bayesian networks\n",
      "14. Linear algebra revision for machine learning and web search\n",
      "15. Singular Value Decomposition - SVD\n",
      "16. Principal Component Analysis - PCA\n",
      "17. Linear prediction\n",
      "18. Least squares and the multivariate Gaussian\n",
      "19. ?\n",
      "20. Cross-validation, big data and regularization\n",
      "21. L1 regularization and the lasso\n",
      "22. Sparse models and variable selection\n",
      "23. Dirichlet and categorical distributions\n",
      "24. Text Classification with Naive Bayes\n",
      "25. Twitter sentiment prediction with Naive Bayes\n",
      "26. Optimization\n",
      "27. Logistic regression\n",
      "28. Neural networks\n",
      "29. Neural nets and backpropagation\n",
      "30. Deep learning\n",
      "31. Decision trees\n",
      "32. Random forests\n",
      "33. Random forests, face detection and Kinect\n",
      "\n",
      "- Texts:\n",
      "    - Russell & Norvig, *Artificial Intelligence: A Modern Approach* http://aima.cs.berkeley.edu\n",
      "    - Hastie, Tibshirani, Friedman, *The Elements of Statistical Learning: Data Mining, Inference, and Prediction*\n",
      "    - Heckerman, \"A Tutorial on Learning with Bayesian Networks\" [Link]( http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=1212A2209A9906C0EE6AE659AA38BC27?doi=10.1.1.15.4522&rep=rep1&type=pdf)\n",
      "    - Murphy, \"A brief introduction to Bayes' Rule\" [Link](http://people.cs.ubc.ca/~murphyk/Bayes/bayesrule.html)\n",
      "    - Wikipedia - Singular Value Decomposition (SVD): [SVD](http://en.wikipedia.org/wiki/Singular_value_decomposition)\n",
      "    - *Linear Algebra: Matrices, Vectors, Determinants*, [Link](http://www.cs.ubc.ca/~nando/340-2009/lectures/linalg.pdf)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Machine Learning 2013**: [Lectures](https://www.youtube.com/playlist?list=PLE6Wd9FR--EdyJ5lbFl8UuGjecvVw66F6) [Homepage](http://www.cs.ubc.ca/~nando/) [Google Scholar](https://scholar.google.com/citations?user=nzEluBwAAAAJ) [Course Page](http://www.cs.ubc.ca/~nando/540-2013/lectures.html)\n",
      "\n",
      "1. Introduction to machine learning\n",
      "2. Linear Prediction\n",
      "3. Maximum likelihood and linear regression\n",
      "4. Ridge, nonlinear regression with basis functions and cross-validation I\n",
      "5. Ridge, nonlinear regression with basis functions and cross-validation II\n",
      "6. Bayesian learning\n",
      "7. Bayesian learning part 2\n",
      "8. Introduction to Gaussian processes for nonlinear regression\n",
      "9. Gaussian processes for nonlinear regression\n",
      "10. Bayesian Optimization, Thompson sampling and multi-armed bandits\n",
      "11. Decision Trees\n",
      "12. Random Forests\n",
      "13. Random forests applications: Object detection and Kinect\n",
      "14. Unconstrained optimization: Gradient descent and Newton's method\n",
      "15. Logistic regression: IRLS and importance sampling\n",
      "16. Neural Networks\n",
      "17. Deep Learning with autoencoders\n",
      "18. Deep Learning II, the Google autoencoders and dropout\n",
      "19. Importance sampling and MCMC I\n",
      "20. Markov chain Monte Carlo (MCMC) II\n",
      "21. Constrained optimization: Lagrangians and duality. Application to penalized maximum likelihood and Lasso\n",
      "\n",
      "- Text:\n",
      "    - Kevin P. Murphy, *Machine Learning: A Probabilistic Perspective*\n",
      "    - Hastie, Tibshirani, Friedman, *The Elements of Statistical Learning: Data Mining, Inference, and Prediction*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deep Learning**: [Lectures](https://www.youtube.com/user/ProfNandoDF/videos) [Homepage](http://www.cs.ubc.ca/~nando/) [Google Scholar](https://scholar.google.com/citations?user=nzEluBwAAAAJ) [Course Page](https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/)\n",
      "\n",
      "1. Introduction\n",
      "2. Linear prediction\n",
      "3. Maximum likelihood\n",
      "4. Regularizers, basis functions and cross-validation I\n",
      "4. Regularizers, basis functions and cross-validation II\n",
      "5. Optimization\n",
      "6. Logistic regression\n",
      "7. Back-propagation and layer-wise design of neural nets (Torch)\n",
      "8. Neural networks and deep learning with Torch\n",
      "9. Convolutional Neural Networks\n",
      "10. Max-margin learning and siamese networks\n",
      "11. Recurrent neural networks and LSTMs\n",
      "12. Hand-writing with recurrent neural networks\n",
      "13. Variational autoencoders and image generation\n",
      "14. Reinforcement learning with direct policy search\n",
      "15. Reinforcement learning with action-value functions\n",
      "\n",
      "- Recommended texts:\n",
      "    - Kevin P. Murphy, *Machine Learning: A Probabilisitc Perspective*, MIT Press 2012. \n",
      "    - Christopher M. Bishop. *Pattern Recognition and Machine Learning*, Springer 2007.\n",
      "    - T. Hastie, R. Tibshirani, and J. Friedman. *The Elements of Statistical Learning*. Springer 2011.\n",
      "    - S. Haykin. *Neural networks and learning machines*. Pearson 2008."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Machine Learning (Andrew Ng)**: [Course Page](http://cs229.stanford.edu/info.html)\n",
      "\n",
      "- Lecture Notes:\n",
      "    1. Supervised, Learning Discriminative Algorithms\n",
      "    2. Generative Algorithms\n",
      "    3. Support Vector Machines\n",
      "    4. Learning Theory\n",
      "    5. Regularization and Model Selection\n",
      "    6. Online Learning and the Perceptron Algorithm\n",
      "    7. Unsupervised Learning, k-means clustering\n",
      "    8. Mixture of Gaussians\n",
      "    9. The EM Algorithm\n",
      "    10. Factor Analysis\n",
      "    11. Principal Component Analysis\n",
      "    12. Independent Component Analysis\n",
      "    13. Reinforcement Learning and Control\n",
      "- Section Notes:\n",
      "    1. Linear Algebra Review and Reference\n",
      "    2. Probability Theory Review\n",
      "    3. Convex Optimization Overview, Part I\n",
      "    4. Convex Optimization Overview, Part II\n",
      "    5. Hidden Markov Models\n",
      "    6. The Multivariate Gaussian Distribution\n",
      "    7. More on Gaussian Distribution\n",
      "    8. Gaussian Processes.\n",
      "\n",
      "- Richard Duda, Peter Hart and David Stork, *Pattern Classification*, 2nd ed. John Wiley & Sons, 2001.\n",
      "- Tom Mitchell, *Machine Learning*. McGraw-Hill, 1997.\n",
      "- Richard Sutton and Andrew Barto, *Reinforcement Learning: An introduction*. MIT Press, 1998\n",
      "- Trevor Hastie, Robert Tibshirani and Jerome Friedman, *The Elements of Statistical Learning*. Springer, 2009"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**MLSS Iceland 2014**: [Home Page](http://mlss2014.hiit.fi)[Slides n Codes](http://mlss2014.hiit.fi/slides.php)\n",
      "\n",
      "1. (Neil Lawrence) Introduction to Machine Learning\n",
      "2. (Yoshua Bengio) Deep Learning\n",
      "3. (Iain Murray) Probabilistic Modeling\n",
      "4. (Amr Ahmed) Big Data and Larges Scale Inference\n",
      "5. (Jean-Philippe Vert) Kernel Methods and Computational Biology\n",
      "6. (Michael Betancourt) Efficient Bayesian Inference with Hamiltonian Monte Carlo\n",
      "7. (Frank Wood) Probabilistic Programming and Bayesian Nonparametrics\n",
      "8. (Ralf Herbich) Applications of Large Scale Graphical Models\n",
      "9. (Jeff Bilmes) Submodularity and Optimization\n",
      "10. (Mark Girolami) Advanced Topics in Markov Chain Monte Carlo\n",
      "11. (Chris Holmes) Robust Inference\n",
      "12. (Timo Koski) Theoretical Issues in Statistical Learning"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Machine Learning and Data Mining**: [Playlist](https://www.youtube.com/playlist?list=PL513vUSBNcuHEk3gsnT1llAP3HkX6PKbB)\n",
      "\n",
      "1. AI & Machine Learning\n",
      "2. Data and Visualization\n",
      "3. Supervised Learning\n",
      "4. Complexity and Overfitting\n",
      "5. Nearest Neighbor (1)\n",
      "6. Nearest Neighbor (2): k-nearest neighbor\n",
      "7. Review: probability\n",
      "8. Bayes Classifiers"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Soft Computing and Machine Learning**: [Course Page](http://rkala.in/softcomputingvideos.php)\n",
      "\n",
      "1. Introduction\n",
      "2. Expert Systems, Machine Learning and Pattern Matching\n",
      "3. Machine learning (cont) and Graph search methods\n",
      "4. Graph Search Methods\n",
      "5. Graph Search Methods (cont) and Clustering\n",
      "6. Clustering (cont), Classification, Functional Approximation and Optimization\n",
      "7. Artificial Neural Networks - Introduction and Architectures\n",
      "8. Artificial Neural Networks - Learning and Back Propagation\n",
      "9. Artificial Neural Networks - Back Propagation and Design Principles\n",
      "10. Radial Basis Function Networks\n",
      "11. Learning Vector Quantization and Self-Organizing Map\n",
      "12. Other Neural and Classification Models\n",
      "13. Evolutionary Computation and Genetic Algorithms\n",
      "14. Genetic Operators\n",
      "15. Genetic Operators and Problem Solving\n",
      "16. Optimization Analysis\n",
      "17. Optimization Analysis (cont) and Design Principles\n",
      "18. Fuzzy Logic\n",
      "19. Fuzzy Operators\n",
      "20. Fuzzy Operators (cont)\n",
      "21. Fuzzy Systems Design Principles\n",
      "22. Particle Swarm Optimization and Ant Colony Optimization\n",
      "23. Other Swarm Intelligence Algorithms\n",
      "24. Other Swarm Intelligence Algorithms and Genetic Programming\n",
      "25. Genetic Programming (cont)\n",
      "26. Grammatical Evolution\n",
      "27. Evolutionary Strategies\n",
      "28. Evolutionary Strategies (cont), Adaptation, and Other Evolutionary Algorithms\n",
      "29. Hybrid Computing and Evolutionary Neural Network\n",
      "30. Variable Architecture and Evolutionary Neural Network\n",
      "31. Grammatical Evolution Based Evolution of Neural Network\n",
      "32. Evolutionary Fuzzy Inference System\n",
      "33. Evolutionary Fuzzy Inference System (cont), Multiple Neural Network Systems\n",
      "34. Neural Network Ensembles\n",
      "35. Modular Neural Networks\n",
      "36. Evolutionary Multiple Neural Networ Systems\n",
      "37. Adaptive Neuro Fuzzy Inference Systems (ANFIS)\n",
      "38. Parallel Evolutionary Algorithms\n",
      "39. Hierarchical Evolutionary Algorithms and Supplementary Topics"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Machine learning coursera** (Andrew Ng) [Playlist](https://www.youtube.com/playlist?list=PLZ9qNFMHZ-A4rycgrgOYma6zxF4BZGGPW)\n",
      "\n",
      "1. What is Machine Learning?\n",
      "2. Supervised Learning\n",
      "3. Unsupervised Learning\n",
      "4. Model Representation\n",
      "5. Cost Function\n",
      "6. Gradient Descent\n",
      "7. Gradient Descent for Linear Regression\n",
      "8. Matrices and Vectors\n",
      "9. Addition and Scalar Multiplication\n",
      "10. Matrix Vector Multiplication\n",
      "11. Matrix Multiplication Properties\n",
      "12. Inverse and Transpose\n",
      "13. Multiple Features\n",
      "14. Gradient Descent with Multiple Variables\n",
      "15. Gradient Descent in Practice - I. Feature Scaling\n",
      "16. Gradient Descent in Practice - II. Learning Rate\n",
      "17. Features and Polynomial Regression\n",
      "18. Normal Equation\n",
      "19. Normal Equation Noninvertibility Optional\n",
      "20. Basic Operations\n",
      "21. Moving Data Around\n",
      "22. Computing on Data\n",
      "23. Plotting Data\n",
      "24. Vectorization\n",
      "25. Classification\n",
      "26. Hypothesis Representation\n",
      "27. Decision Boundary\n",
      "28. Cost Function\n",
      "29. Simplified Cost Function and Gradient Descent\n",
      "30. Advanced Optimization\n",
      "31. Multiclass Classification One vs. all\n",
      "32. The Problem of Overfitting\n",
      "33. Cost Function\n",
      "34. Regularized Linear Regresssion\n",
      "35. Nonlinear Hypothesis\n",
      "36. Neurons in the brain\n",
      "37. Model Representation\n",
      "38. Backpropagation Algorithm\n",
      "39. Gradient Checking\n",
      "40. Random Initialization\n",
      "41. Autonomous Driving\n",
      "42. Model Selection and Train Validation Test Sets\n",
      "43. Diagnosing Bias vs. Variance\n",
      "44. Error analysis\n",
      "45. Data for Machine Learning\n",
      "46. Optimization Objective\n",
      "47. Kernels\n",
      "48. Using an SVM\n",
      "49. Unsupervised Learning Introduction\n",
      "50. K-means Algorithm\n",
      "51. Optimization Objective\n",
      "52. Random Initialization\n",
      "53. Motivation I - Data Compression\n",
      "54. Motivation II - Visualization \n",
      "55. Principal Component Analysis\n",
      "56. Anomaly detection\n",
      "57. Stochastic Gradient Descent \n",
      "58. Online Learning\n",
      "59. Map Reduce and Data Parallelism \n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "** Neural Networks for Machine Learning ** (Geoffrey Hinton) [Playlist](https://www.youtube.com/playlist?list=PLPWgVABcUWuXb-zQiKyqRD91nETdSE5fW)\n",
      "\n",
      "1. Why do we need machine learning?\n",
      "2. What are neural networks\n",
      "3. Some simple models of neurons\n",
      "4. A simple example of learning\n",
      "5. Three types of learning\n",
      "6. Types of neural network architectures\n",
      "7. Perceptrons - the first generation of neural networks\n",
      "8. A geometrical view of perceptrons\n",
      "9. Why the learning workds\n",
      "10. What perceptrons can't do\n",
      "11. Learning the weights of a linear neuron\n",
      "12. The error surface for a linear neuron\n",
      "13. Learning the weights of a logistic output neuron\n",
      "14. The backpropagation algorithm\n",
      "15. Using the derivatives computed by backpropagation\n",
      "16. Learning to predict the next word\n",
      "17. A brief diversion into cognitive science\n",
      "18. Another diversion - The softmax output function\n",
      "19. Neuro-probabilistic language models\n",
      "20. Ways to deal with the large number of possible outputs\n",
      "21. Why object recognition is difficult\n",
      "22. Achieving viewpoint invariance\n",
      "23. Convolutional nets for digit recognition\n",
      "24. Convolutional nets for object recognition\n",
      "25. Overview of mini-batch gradient descent\n",
      "26. A bag of tricks for mini-batch gradient descent\n",
      "27. The momentum method\n",
      "28. Adaptive learning rates for each connection\n",
      "29. Rmsprop Divide the gradient by a running average of its recent magnitude\n",
      "30. Modeling sequences - A brief overview\n",
      "31. Training RNNs with backpropagation\n",
      "32. A toy example of training a RNN\n",
      "33. Why it is difficult to train an RNN\n",
      "34. Long-term Short-term memory\n",
      "35. A brief overview of Hessian Free optimization\n",
      "36. Modling character strings with multiplicative connections\n",
      "37. Learning to predict the next character using HF\n",
      "38. Echo State Networks\n",
      "39. Overview of ways to improve generalization\n",
      "40. Limiting the size of the weights\n",
      "41. Using noise as a regularizer\n",
      "42. Introduction to the full Bayesian Approach\n",
      "\n",
      "**Deep Learning and Neural Networks** (Kevin Duh) [Course Page](https://www.youtube.com/redirect?q=http%3A%2F%2Fcl.naist.jp%2F~kevinduh%2Fa%2Fdeep2014%2F&redir_token=q81t32rCXc14aJJ_rxZtBeNwNad8MTQyNTIzODIzM0AxNDI1MTUxODMz)\n",
      "\n",
      "1. Machine Learning background and Neural Networks\n",
      "2. Two Types of Deep Architectures: Deep Belief Nets (DBN) and Stacked Auto-Encoders (SAE)\n",
      "3. Applications in COmputer Vision, Speech Recognition and Language Modeling\n",
      "4. Advanced Topics in Optimization\n",
      "\n",
      "**Introduction to Statistical Learning** (Kevin Duh) [Course Page](http://ssli.ee.washington.edu/courses/ee511/schedule.html)\n",
      "\n",
      "1. issues in statistical learning\n",
      "2. classification with likelihood functions\n",
      "3. learning parametric models via MLE\n",
      "4. Sufficient statistic, EM Algorithm\n",
      "5. EM Mixture example, Bayesian Alternatives to MLE for training of parametric distributions\n",
      "6. classification by linear functions\n",
      "7. online learning, linear regression\n",
      "8. Non-parametric Classification and Estimation\n",
      "9. Feature reduction and selection\n",
      "10. Performance Estimation\n",
      "11. Model comparison, model selection\n",
      "12. Categorical variables, decision trees\n",
      "13. SVMs\n",
      "14. Model combination\n",
      "15. Artificial Neural Networks\n",
      "16. Unsupervised Learning (clustering) for continuous inputs\n",
      "17. semi-supervised learning"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Learning from Data** (Yaser Abu-Mostafa) [Lectures](http://work.caltech.edu/lectures.html#lectures) [Videos](https://www.youtube.com/playlist?list=PLD63A284B7615313A)\n",
      "\n",
      "1. The Learning Problem\n",
      "2. Is Learning Feasible\n",
      "3. The Linear Model I\n",
      "4. Error and Noise\n",
      "5. Training versus Testing\n",
      "6. Theory of Generalization\n",
      "7. The VC Dimension\n",
      "8. Bias-Variance Tradeoff\n",
      "9. The Linear Model II\n",
      "10. Neural Networks\n",
      "11. Overfitting\n",
      "12. Regularization\n",
      "13. Validation\n",
      "14. Support Vector Machines\n",
      "15. Kernel Methods\n",
      "16. Radial Basis Functions\n",
      "17. The Three Learning Principles\n",
      "18. Epilogue\n",
      "\n",
      "- Text: : *Learning from Data: A Short Course* [amazon link](http://work.caltech.edu/textbook.html)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Statistics and Data Analysis in MATLAB** [Lecture Notes and Video](http://artsci.wustl.edu/~kkay/psych5007/)\n",
      "\n",
      "1. MATLAB Basics\n",
      "2. Probability distributions and error bars\n",
      "3. Hypothesis testing and correlation\n",
      "4. Model specification\n",
      "5. Model fitting\n",
      "6. MATLAB examples\n",
      "7. Model accuracy\n",
      "8. Model reliability\n",
      "9. MATLAB examples\n",
      "10. Classification\n",
      "11. MATLAB examples"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "** Data Mining with WEKA ** [Course Page](https://weka.waikato.ac.nz/dataminingwithweka/preview)\n",
      "\n",
      "1. Getting started with WEKA\n",
      "2. Evaluation\n",
      "3. Simple classifiers\n",
      "    - overfitting\n",
      "    - using probabilities\n",
      "    - decision trees\n",
      "    - pruning decision trees\n",
      "    - nearest neighbor\n",
      "4. More classifiers\n",
      "    - classification bounaries\n",
      "    - linear regression\n",
      "    - classification by regression\n",
      "    - logistic regression\n",
      "    - support vector machines\n",
      "    - ensemble learning\n",
      "5. Putting it all together\n",
      "\n",
      "** More Data Mining with WEKA ** [Course Page](https://weka.waikato.ac.nz/moredataminingwithweka/preview)\n",
      "\n",
      "1. Exploring WEKA's intergaces, and working with big data\n",
      "2. Discretization and text classification\n",
      "3. Classification rules, association rules, and clustering\n",
      "4. Selecting attributes and counting the cost\n",
      "5. Neural networks, learning curves, and performance optimization\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- **Machine Learning**\n",
      "    - Historical Perspective\n",
      "        - Biological motivations: the McCulloch and Pitts neuron, Hebbian learning.\n",
      "        - Statistical motivations\n",
      "    - Theory\n",
      "        - Generalisation: What is learning?\n",
      "        - The power of machine learning methods: what is a learning algorithm? what can they do?\n",
      "    - Probability\n",
      "        - Probability as representation of uncertainty in models and data\n",
      "        - Bayes Theorem and its applications\n",
      "        - Law of large numbers and the Gaussian distribution\n",
      "        - Markov and graphical models\n",
      "    - Supervised Learning\n",
      "        - Classification using Bayesian principles\n",
      "        - Perceptron Learning\n",
      "        - Support Vector Machines and Kernel methods\n",
      "        - Neural networks/multi-layer perceptrons (MLP)\n",
      "        - Features and discriminant analysis\n",
      "    - Linear Algebra\n",
      "        - Using matrices to find solutions of linear equations\n",
      "        - Properties of matrices and vector spaces\n",
      "        - Eigenvalues, eigenvectors and singular value decomposition\n",
      "    - Data handling and unsupervised learning\n",
      "        - Principal Components Analysis (PCA)\n",
      "        - Blind source separation using Independent Components Analysis (ICA)\n",
      "        - K-Means clustering\n",
      "        - Spectral clustering\n",
      "        - Manifold learning\n",
      "    - Regression and Model-fitting Techniques\n",
      "        - Linear regression\n",
      "        - Polynomial Fitting\n",
      "        - Kernel Based Networks\n",
      "    - Optimisation\n",
      "        - Convexity\n",
      "        - 1-D minimisation\n",
      "        - Gradient methods in higher dimensions\n",
      "        - Constrained optimisation\n",
      "        - Dynamic Programming\n",
      "    - Case Studies\n",
      "        - Example applications: Speech, Vision, Natural Language, Bioinformatics.\n",
      "\n",
      "- Text:\n",
      "    - Bishop, Christopher M., Pattern Recognition and Machine Learning, Springer, 2006.\n",
      "    - Murphy, Kevin, Machine Learning: A Probabilistic Perspective, MIT Press, 2012\n",
      "    - Barber, David, Bayesian Reasoning and Machine Learning, Cambrige UP, 2012\n",
      "    - Mackay, David J. C., Information Theory, Inference and Learning Algorithms \n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Statistical Learning Theory and Applications** [Course Page](http://www.mit.edu/~9.520/fall14/#readings)\n",
      "\n",
      "1. The Course at a Glance\n",
      "2. The Learning Problem and Regularization [Scribe Notes](http://www.mit.edu/~9.520/scribe-notes/class02_scrb_sara.pdf)\n",
      "    - Cucker and Smale. On the mathematical foundations of learning. Bulletin of the American Mathematical Society, 2002. [paper](http://www.mit.edu/~9.520/Papers/cuckersmale.ps)\n",
      "    - Evgeniou, Pontil and Poggio. Regularization Networks and Support Vector Machines Advances in Computational Mathematics, 2000. [paper](http://cbcl.mit.edu/projects/cbcl/publications/ps/evgeniou-reviewall.pdf)\n",
      "    - Vapnik. The Nature of Statistical Learning Theory. Wiley & Sons, 1995.\n",
      "3. Math Camp\n",
      "    - Functional Analysis\n",
      "    - Probability\n",
      "4. Reproducing Kernel Hilbert Spaces [Scribe Notes](http://www.mit.edu/~9.520/scribe-notes/class03_gdurett.pdf)\n",
      "    - Aronszajn. Theory of reproducing kernels. Transactions of the American Mathematical Society, 686, 337-404, 1950. [paper](http://www.mit.edu/~9.520/Papers/aron.pdf)\n",
      "    - Cucker and Smale. On the mathematical foundations of learning. Bulletin of the American Mathematical Society, 2002. [paper](http://www.mit.edu/~9.520/Papers/cuckersmale.ps)\n",
      "    - Evgeniou, Pontil and Poggio. Regularization Networks and Support Vector Machines Advances in Computational Mathematics, 2000.  [paper](http://cbcl.mit.edu/projects/cbcl/publications/ps/evgeniou-reviewall.pdf)\n",
      "    - Girosi, F. An Equivalence between Sparse Approximation and Support Vector Machines. Neural Computation, Vol. 10, 1455-1480, 1998. (Appendix A) [paper](http://cbcl.mit.edu/projects/cbcl/publications/ps/svm.ps)\n",
      "    - Wahba, G. Spline Models for Observational Data Series in Applied Mathematics, Vol. 59, SIAM, 1990. (Chapter 1)\n",
      "    - Berlinet, A. and Thomas-Agnan, C. Reproducing Kernel Hilbert Spaces in Probability and Statistics Kluwer Academic Publishers, 2004.\n",
      "5. Dictionaries, Feature Maps and Mercer Theorem\n",
      "6. Tikhonov Regularization and the Representer Theorem\n",
      "    - De Vito E., Rosasco L., Caponnetto A., Piana M. and Verri A. Some Properties of Regularized Kernel Methods. The Journal of Machine Learning Research, Vol. 5, 2004 [paper](http://web.mit.edu/lrosasco/www/publications/representer.pdf)\n",
      "    - Scholkopf B., Herbrich R., Smola AJ. A Generalized Representer Theorem 14th Conf. on Computational Learning Theory (COLT), 2001 [paper](http://link.springer.com/chapter/10.1007/3-540-44581-1_27)\n",
      "7. Regularized Least Squares\n",
      "    - Rifkin. Everything Old Is New Again: A Fresh Look at Historical Approaches in Machine Learning. MIT Ph.D. Thesis, 2002. [paper](http://web.mit.edu/~9.520/www/Papers/thesis-rifkin.pdf)\n",
      "    - Evgeniou, Pontil and Poggio. Regularization Networks and Support Vector Machines Advances in Computational Mathematics, 2000. [paper](http://cbcl.mit.edu/projects/cbcl/publications/ps/evgeniou-reviewall.pdf)\n",
      "    - Rifkin, R.,. and R.A. Lippert.Notes on Regularized Least-Squares, CBCL Paper #268/AI Technical Report #2007-019, Massachusetts Institute of Technology, Cambridge, MA, May, 2007 [paper](http://cbcl.mit.edu/projects/cbcl/publications/ps/MIT-CSAIL-TR-2007-025.pdf)\n",
      "    - V. N. Vapnik. The Nature of Statistical Learning Theory. Springer, 1995.\n",
      "8. Logistic Regression and Support Vector Machines\n",
      "    - Rifkin. Everything Old Is New Again: A Fresh Look at Historical Approaches in Machine Learning. MIT Ph.D. Thesis, 2002. [paper](http://web.mit.edu/~9.520/www/Papers/thesis-rifkin.pdf)\n",
      "    - Evgeniou, Pontil and Poggio. Regularization Networks and Support Vector Machines Advances in Computational Mathematics, 2000. [paper](http://cbcl.mit.edu/projects/cbcl/publications/ps/evgeniou-reviewall.pdf)\n",
      "9. Iterative Regularization via Early Stopping\n",
      "    - Frank Bauer, Sergei Pereverzev, and Lorenzo Rosasco. On Regularization Algorithms in Learning Theory. Journal of complexity, 23(1):52-72, 2007. [paper](http://www.sciencedirect.com/science/article/pii/S0885064X06000781)\n",
      "    - Lo Gerfo, Lorenzo Rosasco, Francesca Odone, E Vito, and Alessandro Verri. Spectral Algorithms for Supervised Learning. Neural Computation, 20(7):1873-1897, 2008. [paper](http://www.mitpressjournals.org/doi/abs/10.1162/neco.2008.05-07-517#.VPUtKSmrkxI)\n",
      "    - Yuan Yao, Lorenzo Rosasco, and Andrea Caponnetto. On Early Stopping in Gradient Descent Learning. Constructive Approximation, 26(2):289-315, Aug. 2007. [paper](http://link.springer.com/article/10.1007/s00365-006-0663-2)\n",
      "10. Sparsity Based Regularization\n",
      "    - De Mol, C., E. De Vito and L. Rosasco, Elastic-Net Regularization in Learning Theory, Journal of Complexity 25(2), April 2009 (and references therein) (arXiv), (MIT CSAIL Technical Report #TR-2008-046) [paper](http://arxiv.org/pdf/0807.3423v1.pdf)\n",
      "11. Proximal Methods\n",
      "12. Structured Sparsity Regularization\n",
      "13. Multiple Kernel Learning\n",
      "14. On-line Learning\n",
      "    - H. J. Kushner and G. Yin Stochastic Approximation, Recursive Algorithms and Applications. 2nd Edition, Springer-Verlag, New York, 2003, [Applications of Mathematics, Volume 35 Nicolo Cesa-Bianchi and Gabor Lugosi. Prediction, learning and games. Cambridge University Press 2006. [collection](http://www.springer.com/us/book/9780387008943)\n",
      "15. Generalization Bounds, Intro to Stability\n",
      "    - V. N. Vapnik. Statistical Learning Theory. Wiley, 1998.\n",
      "    - F. Cucker and S. Smale. On The Mathematical Foundations of Learning. Bulletin of the American Mathematical Society, 2002.\n",
      "    - O. Bousquet and A. Elisseeff. Stability and Generalization. Journal of Machine Learning Research, 2002. [paper](http://www.mit.edu/~9.520/Papers/bousquet02a.pdf)\n",
      "16. Stability of Tikhonov Regularization\n",
      "17. Consistency, Learnability and Regularization\n",
      "18. Regularization Paramter Choice\n",
      "19. Manifold Regularization\n",
      "    - M. Belkin, P. Niyogi. Semi-supervised Learning on Riemannian Manifolds. Machine Learning, 56, Special Issue on Clustering, 209-239, 2004. [paper](http://www.mit.edu/~9.520/Papers/Belkin-ML-04.pdf)\n",
      "    - M. Belkin, P. Niyogi, V. Sindhwani. On Manifold Regularization. AISTATS 2005. [paper](http://www.mit.edu/~9.520/Papers/Belkin-AISTATS-05.pdf)\n",
      "20. Regularization for Multi-Ouput Learning I\n",
      "    - T. Evgeniou and C.A. Micchelli and M. Pontil Learning multiple tasks with kernel methods, Journal of Machine Learning Research, 6, 615-637, 2005. [paper](http://jmlr.csail.mit.edu/papers/volume6/evgeniou05a/evgeniou05a.pdf)\n",
      "    - M. Pontil and C.A. Micchelli. Kernels for multi-task learning, Advances in Neural Information Processing Systems (NIPS), 2004. [paper](http://www0.cs.ucl.ac.uk/staff/M.Pontil/reading/kernels4MTL.pdf)\n",
      "    - M. Pontil and C.A. Micchelli. On learning vector-valued functions, Neural Computation, 17:177-204, 2005. [paper](http://www0.cs.ucl.ac.uk/staff/M.Pontil/reading/vecval.pdf)\n",
      "21. Regularization for Multi-Output Learning II\n",
      "22. Learning Data Representations: from Fourier to Dictionary Learning\n",
      "23. Learning Data Representation: Deep Learning I\n",
      "    - Anselmi F, J.Z. Leibo, L. Rosasco, J. Mutch, A. Tacchetti, and T. Poggio, \"Unsupervised learning of invariant representations with low sample complexity: the magic of sensory cortex or a new framework for machine learning?\", CBMM Memo 001, Nov. 2013. [paper](http://arxiv.org/pdf/1311.4158v5.pdf)\n",
      "    - Anselmi F., J.Z. Leibo, L. Rosasco, J. Mutch, A. Tacchetti, and T. Poggio, \"Magic Materials: a theory of deep hierarchical architectures for learning sensory representations\", CBCL paper, MIT, Cambridge, MA, Apr. 2013. [paper](http://cbcl.mit.edu/publications/ps/Magic_working_paper_Sept27_2013.pdf)\n",
      "    - Anselmi F. and T. Poggio, \"Representation learning in sensory cortex: a theory\", CBMM Memo 026, Nov. 2014.\n",
      "24. Learning Data Representation: Deep Learning II [paper](https://cbmm.mit.edu/publications/representation-learning-sensory-cortex-theory)\n",
      "\n",
      "- Primary References\n",
      "    - Bousquet, O., S. Boucheron and G. Lugosi. [Introduction to Statistical Learning Theory](http://www.kyb.mpg.de/fileadmin/user_upload/files/publications/pdfs/pdf2819.pdf). Advanced Lectures on Machine Learning Lecture Notes in Artificial Intelligence 3176, 169-207. (Eds.) Bousquet, O., U. von Luxburg and G. Ratsch, Springer, Heidelberg, Germany (2004)\n",
      "    - F. Cucker and S. Smale. [On The Mathematical Foundations of Learning](http://www.mit.edu/~9.520/Papers/cuckersmale.ps). Bulletin of the American Mathematical Society, 2002.\n",
      "    - F. Cucker and D-X. Zhou. Learning theory: an approximation theory viewpoint. Cambridge Monographs on Applied and Computational Mathematics. Cambridge University Press, Cambridge, 2007.\n",
      "    - L. Devroye, L. Gyorfi, and G. Lugosi. A Probabilistic Theory of Pattern Recognition. Springer, 1997.\n",
      "    - T. Evgeniou and M. Pontil and T. Poggio. Regularization Networks and Support Vector Machines. Advances in Computational Mathematics, 2000.\n",
      "    - T. Poggio and S. Smale. [The Mathematics of Learning: Dealing with Data](http://cbcl.mit.edu/projects/cbcl/publications/ps/notices-ams2003refs.pdf). Notices of the AMS, 2003\n",
      "    - I. Steinwart and A. Christmann. Support vector machines. Springer, New York, 2008.\n",
      "    - V. N. Vapnik. Statistical Learning Theory. Wiley, 1998.\n",
      "    - V. N. Vapnik. The Nature of Statistical Learning Theory. Springer, 1995.\n",
      "    - N. Cristianini and J. Shawe-Taylor. Introduction To Support Vector Machines. Cambridge, 2000.\n",
      "\n",
      "- Background Mathematics References\n",
      "    - A. N. Kolmogorov and S. V. Fomin, Introductory Real Analysis, Dover Publications, 1975.\n",
      "    - A. N. Kolmogorov and S. V. Fomin, Elements of the Theory of Functions and Functional Analysis, Dover Publications, 1999.\n",
      "    - Luenberger, Optimization by Vector Space Methods, Wiley, 1969.\n",
      "\n",
      "- Neuroscience Related References\n",
      "    - Serre, T., L. Wolf, S. Bileschi, M. Riesenhuber and T. Poggio. \"Object Recognition with Cortex-like Mechanisms\", IEEE Transactions on Pattern Analysis and Machine Intelligence, 29, 3, 411-426, 2007. [paper](http://cbcl.mit.edu/projects/cbcl/publications/ps/serre-wolf-poggio-PAMI-07.pdf)\n",
      "    - Serre, T., A. Oliva and T. Poggio.\"A Feedforward Architecture Accounts for Rapid Categorization\", Proceedings of the National Academy of Sciences (PNAS), Vol. 104, No. 15, 6424-6429, 2007. [paper](http://cbcl.mit.edu/projects/cbcl/publications/ps/serre-PNAS-4-07.pdf)\n",
      "    - S. Smale, L. Rosasco, J. Bouvrie, A. Caponnetto, and T. Poggio. \"Mathematics of the Neural Response\", Foundations of Computational Mathematics, Vol. 10, 1, 67-91, June 2009. [paper](http://dspace.mit.edu/bitstream/handle/1721.1/43713/MIT-CSAIL-TR-2008-070.pdf)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- **Convolutional Neural Networks for Visual Recognition** [Page](http://cs231n.github.io)\n",
      "    - Module 1: Neural Networks\n",
      "        - Image Classification: Data-driven Approach, k-Nearest Neighbor, train/val/test splits\n",
      "            - L1/L2 distances, hyperparameter search, cross-validation\n",
      "        - Linear classification: Support Vector Machine, Softmax\n",
      "            - parameteric approach, bias trick, hinge loss, cross-entropy loss, L2 regularization, web demo\n",
      "        - Optimization: Stochastic Gradient Descent\n",
      "            - optimization landscapes, local search, learning rate, analytic/numerical gradient\n",
      "        - Backpropagation, Intuitions\n",
      "            - chain rule interpretation, real-valued circuits, patterns in gradient flow\n",
      "        - Neural Networks Part 1: Setting up the Architecture\n",
      "            - model of a biological neuron, activation functions, neural net architecture, representational power\n",
      "        - Neural Networks Part 2: Setting up the Data and the Loss\n",
      "            - preprocessing, weight initialization, regularization, dropout, loss functions in the wild\n",
      "        - Neural Networks Part 3: Learning and Evaluation\n",
      "            - gradient checks, sanity checks, babysitting the learning process, momentum (+nesterov), second-order methods, Adagrad/RMSprop, hyperparameter optimization, model ensembles\n",
      "    - Module 2: Convolutional Neural Networks\n",
      "        - Convolutional Neural Networks: Architectures, Convolution / Pooling Layers\n",
      "            - layers, spatial arrangement, layer patterns, layer sizing patterns, AlexNet/ZFNet/VGGNet case studies, computational - considerations\n",
      "        - Understanding and Visualizing Convolutional Neural Networks\n",
      "        - Transfer Learning and Fine-tuning Convolutional Neural Networks\n",
      "        - ConvNet Tips and Tricks: squeezing out the last few percent\n",
      "    - Module 3: ConvNets in the wild\n",
      "        - Other Visual Recognition Tasks: Localization, Detection, Segmentation\n",
      "        - ConvNets in Practice: Distributed Training, GPU bottlenecks, Libraries\n",
      "        \n",
      "- **Neural Networks and Deep Learning** [Online Book](http://neuralnetworksanddeeplearning.com/index.html)        \n",
      "    1. Neural Networks and Deep Learning\n",
      "        - What this book is about\n",
      "        - On the exercises and the problems\n",
      "    2. Using neural nets to recognize handwritten digits\n",
      "        - Perceptron\n",
      "        - Sigmoid neurons\n",
      "        - The architecture of neural networks\n",
      "        - A simple network to classify handwritten digits\n",
      "        - Learning with gradient descent\n",
      "        - Implementing our network to classify digits\n",
      "        - Toward deep learning\n",
      "    3. How the backpropagation algorithm works\n",
      "        - Warm up: a fast matrix-based approach to computing the output from a neural network\n",
      "        - The two assumptions we need about the cost function\n",
      "        - The Hadamard product $s \\odot t$\n",
      "        - The four fundamental equations behind backpropagation\n",
      "        - Proof of the four fundamental equations\n",
      "        - The backpropagation algorithm\n",
      "        - The code for backpropagation\n",
      "        - In what sense is backpropagation a fast algorithm?\n",
      "        - Backpropagation: the big picture\n",
      "    4. Improving the way neural networks learn\n",
      "        - The cross-entropy cost function\n",
      "        - Overfitting and regularization\n",
      "        - Weight initialization\n",
      "        - Handwriting recognition revisited: the code\n",
      "        - How to choose a neural network's hyper-parameters?\n",
      "        - Other techniques\n",
      "    5. A visual proof that neural nets can compute any function\n",
      "        - Two caveats\n",
      "        - Universality with one input and one output\n",
      "        - Many input variables\n",
      "        - Extension beyond sigmoid neurons\n",
      "        - Fixing up the step functions\n",
      "    6. What are deep neural networks hard to train?\n",
      "        - The vanishing gradient problem\n",
      "        - What's causing the vanishing gradient problem? Unstable gradients in deep neural nets\n",
      "        - Unstable gradients in more complex networks\n",
      "        - Other obstacles to deep learning\n",
      "    7. Deep learning\n",
      "        - Convolutional neural networks\n",
      "        - Pretraining\n",
      "        - Recurrent neural networks, Boltzmann machines, and other models\n",
      "        - Is there a universal thinking algorithm?\n",
      "        - On the future of neural networks"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- **Practical Machine Learning** [page](http://www.cs.berkeley.edu/%7Ejordan/courses/294-fall09/)\n",
      "    1. Tutorial\n",
      "    2. Classification\n",
      "    3. Regression\n",
      "    4. Clustering\n",
      "    5. Dimensionality Reduction\n",
      "    6. Feature Selection\n",
      "    7. Hidden Markov Models, graphical models\n",
      "    8. Collaborative Filtering\n",
      "    9. Active learning, experimental design\n",
      "    10. Reinforcement Learning\n",
      "    11. Bootstrap, cross-validation, ROC plots\n",
      "    12. Time series, sequential hypothesis testing, anomaly detection\n",
      "    13. Bayesian nonparametric methods\n",
      "    14. Optimization methods for learning\n",
      "    15. Texts:\n",
      "        - Hastie, Tibshirani and Friedman. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Book's web site\n",
      "        - Witten and Frank. Data Mining: Practical Machine Learning Tools and Techniques. Book's web site\n",
      "        - Andrew Moore's Tutorials are a collection of PDF tutorials on many of the topics that will be covered in the class\n",
      "            - [Location with Links](http://www.autonlab.org/tutorials/list.html)\n",
      "            - Decision Trees\n",
      "            - Information Gain\n",
      "            - Probability for Data Miners\n",
      "            - Probability Density Functions\n",
      "            - Gaussians\n",
      "            - Maximum Likelihood Estimation\n",
      "            - Gaussian Bayes Classifiers\n",
      "            - Cross-Validation\n",
      "            - Neural Networks\n",
      "            - Instance-based learning (aka Case-based or Memory-based or non-parametric)\n",
      "            - Eight Regression Algorithms\n",
      "            - Predicting Real-valued Outputs: An introduction to regression\n",
      "            - Bayesian Networks\n",
      "            - Inference in Bayesian Networks (by Scott Davies and Andrew Moore)\n",
      "            - Learning Bayesian Networks\n",
      "            - A Short Intro to Naive Bayesian Classifiers\n",
      "            - Short Overview of Bayes Nets\n",
      "            - Gaussian Mixture Models\n",
      "            - K-means and Hierarchical Clustering\n",
      "            - Hidden Markov Models\n",
      "            - VC dimension\n",
      "            - Support Vector Machines\n",
      "            - PAC Learning\n",
      "            - Markov Decision Processes\n",
      "            - Reinforcement Learning\n",
      "            - Biosurveillance: An example\n",
      "            - Elementary probability and Naive Bayes classifiers\n",
      "            - Spatial Surveillance\n",
      "            - Time Series Methods\n",
      "            - Game Tree Search Algorithms, including Alpha-Beta Search\n",
      "            - Zero-Sum Game Theory\n",
      "            - Non-zero-sum Game Theory\n",
      "            - Introductory overview of time-series-based anomaly detection algorithms\n",
      "            - AI Class introduction\n",
      "            - Search Algorithms\n",
      "            - A-star Heuristic Search\n",
      "            - Constraint Satisfaction Algorithms, with applications in Computer Vision and Scheduling\n",
      "            - Robot Motion Planning\n",
      "            - HillClimbing, Simulated Annealing and Genetic Algorithms"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- **Neural Machine Learning and Data Mining II** [page](http://www.ece.rice.edu/~erzsebet/ANNcourseII.html)\n",
      "    1. Introduction, orientation\n",
      "    2. Review part of Neural Machine Learning I\n",
      "    3. Kohonen Maps (SOMs) and Their Interpretation\n",
      "    4. Variants of SOMs\n",
      "    5. Self-Organizing Maps for High-Dimensional and Complex Data\n",
      "    6. Unsupervised Learning as Support for Supervised Classification\n",
      "    7. Evaluation of Clustering Quality, and Classification Accuracy\n",
      "    8. Dimension Assessment and Non-linear Dimension Reduction\n",
      "    9. Metrics for Learning, and Learning of Metrics\n",
      "    10. BSS and ICA\n",
      "    11. Unsupervised Learning Using Kernel Methods\n",
      "    12. Texts:\n",
      "        - Simon Haykin: Neural Networks. A Comprehensive Foundation. McMillan, New Jersey, 1999. (2nd Edition)\n",
      "        - Teuvo Kohonen: Self-organizing Maps (Springer Series in Information Sciences S.). Springer-Verlag, 2001 (3rd Edition, ISBN: 3540679219)\n",
      "        - Frederick Ham and Ivica Kostanic: Principles of Neurocomputing for Science & Engineering. McGraw-Hill, 2001."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- **Neural Machine Learning I** [page](http://www.ece.rice.edu/~erzsebet/ANNcourse.html)\n",
      "    1. Orientation, Review of the basics\n",
      "    2. Train a single PE\n",
      "    3. Review of basics\n",
      "    4. Associative memory\n",
      "    5. Simple perceptron\n",
      "    6. Multilayer Perceptron & Back Propagation\n",
      "    7. Issues with Backpropagation\n",
      "    8. Scalings, Momentum\n",
      "    9. \\# of training samples; Vapnik-Chervonekis dimension\n",
      "    10. Network pruning\n",
      "    11. Competitive Learning: Self-Organizing Maps\n",
      "\n",
      "- **Methods for Applied Statistics: Unsupervised Learning** [page](http://web.stanford.edu/%7Elmackey/stats306b/)\n",
      "    1. Unsupervised v. Supervised Learning; Clustering with k-means and k-medoids\n",
      "    2. Gaussian mixture models: Expectation-Maximisation\n",
      "    3. General Mixture Modeling\n",
      "    4. Discrete Hidden Markov Models\n",
      "    5. Hierarchical Clustering\n",
      "    6. Spectral clustering\n",
      "    7. Linear Dimensionality Reduction via Principal Component Analysis\n",
      "    8. Kernel PCA\n",
      "    9. Factor Analysis\n",
      "    10. Linear Gaussian State-Space Models and Kalman Filtering\n",
      "    11. Linear Gaussian SSMs\n",
      "    12. Independent Component Analysis; Canonica Correlation Analysis\n",
      "    13. Sparse Unsupervised Learning\n",
      "    14. Unsupervised Deep Learning\n",
      "    15. Learning with Missing Data\n",
      "    16. Unsupervised Learning with Missing Data\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- **Machine Learning Summer School** [Videos](https://www.youtube.com/playlist?list=PLqJm7Rc5-EXFv6RXaPZzzlzo93Hl0v91E) [Video Lectures](http://videolectures.net/mlss09uk_cambridge/)\n",
      "    0. What is Machine Learning? - Bernhard Scholkopf\n",
      "    1. Graphical Models - Christopher Bishop\n",
      "    2. Bayesian Inference - Zoubin Ghahramani\n",
      "    3. Optimization - Stephen Wright\n",
      "    4. Kernels - Bernhard Scholkopf\n",
      "    5. Causality - Bernhard Scholkopf\n",
      "    6. Multilayer Networks - Leon Bottou\n",
      "    7. Sparse Models - Matthias Seeger\n",
      "    8. Gaussian Processes - Philipp Hennig\n",
      "    9. Structured Output Prediction - Thomas Hofmann\n",
      "    10. Learning Theory - Ingo Steinwart\n",
      "    11. Bayesian Nonparametrics\n",
      "    12. Bandits, Active Learning, Bayesian RL and Global Optimization - Marc Toussaaint\n",
      "    13. Machine Learning for Computational Biology\n",
      "    14. Robotics - Stefan Schaal\n",
      "    15. Graphical Models - Zoubin Gharhamani\n",
      "    16. Markov Chain Monte Carlo - Iain Murray\n",
      "    17. Information Theory - David Mckay\n",
      "    18. Kernel Methods - Bernhard Scholkopf\n",
      "    19. Approximate Inference - Tom Minka\n",
      "    20. Topic Models - David Blei\n",
      "    21. Gaussian Processes - Rasmussen\n",
      "    22. Convex Optimization - Vandenberghe\n",
      "    23. Learning Theory - John Shawe-Taylor\n",
      "    24. Computer Vision - Andrew Blake\n",
      "    25. Nonparametric Bayesian Models - Teh\n",
      "    26. Machine Learning and Cognitive Science - Tenenbaum\n",
      "    27. Reinforcement Learning - Littman\n",
      "    28. Foundation of Nonparametric Bayesian Methods\n",
      "    29. Deep Belief Networks\n",
      "    30. Particle Filters\n",
      "    31. Causality\n",
      "    32. Information Retrieval\n",
      "    33. Bayesian or Frequentist, Which Are You?\n",
      "    "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- **Machine Learning** [page](http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-867-machine-learning-fall-2006/index.htm)\n",
      "    1. Introduction, linear classification, perception update rule\n",
      "    2. Perceptron convergence, generalization\n",
      "    3. Maximum margin classification\n",
      "    4. Classification errors, regularization, logistic regression\n",
      "    5. Linear regression, estimator bias and variance, active learning\n",
      "    6. Active learning, non-linear predictions, kernels\n",
      "    7. Kernel regression, kernels\n",
      "    8. Support vector machines (SVMs) and kernels, kernel optimization\n",
      "    9. Model selection\n",
      "    10. Model selection criteria\n",
      "    11. Description length, feature selection\n",
      "    12. Combining classifiers, boosting\n",
      "    13. Boosting, margin, and complexity\n",
      "    14. Margin and generalization, mixture models\n",
      "    15. Mixtures and the expectation maximization (EM) algorithm\n",
      "    16. EM, regularization, clustering\n",
      "    17. Clustering\n",
      "    18. Spectral clustering, Markov models\n",
      "    19. Hidden Markov models\n",
      "    20. Bayesian networks\n",
      "    21. Learning Bayesian networks\n",
      "    22. Probabilistic inference\n",
      "    23. Current problems in machine learning"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- **Undergraduate Machine Learning Introduction** [page](http://www.cs.toronto.edu/~tijmen/csc321/lecture_notes.shtml)\n",
      "    1. Artificial Neurons\n",
      "        - Why do we need machine learning?\n",
      "        - What are neural networks?\n",
      "        - Some simple models of neurons\n",
      "        - A simple example of learing\n",
      "        - Three types of learning\n",
      "    2. Neural Networks\n",
      "        - Types of neural network architecture\n",
      "        - Perceptrons: The first generation of neural networks\n",
      "        - A geometrical view of perceptrons\n",
      "        - Why the learning works\n",
      "        - What perceptrons can't do\n",
      "    3. Learning weights and backpropagation\n",
      "        - Learning the weights of a linear neuron\n",
      "        - The error surgace for a linear neuron\n",
      "        - Learning the weights of a logistic output neuron\n",
      "        - The backpropagation algorithm\n",
      "        - Using the derivatives computed by backpropagation\n",
      "    4. Language models\n",
      "        - Learning to predict the next word\n",
      "        - A brief diversion into cognitive science\n",
      "        - Another diversion: The softmax output fuction\n",
      "        - Neuro-probabilistic language models\n",
      "        - Ways to deal with the large number of possible outputs\n",
      "    5. Object Recognition and CNNs\n",
      "        - Why object recognition is difficult\n",
      "        - Achieving viewpoint invariance\n",
      "        - Convolutional nets for digit recognition\n",
      "        - Convolutional nets for object recognition\n",
      "    6. Gradient Descent, momentum & adaptive learning rates\n",
      "        - Overview of mini-batch gradient descent\n",
      "        - A bag of tricks for mini-batch gradient descent\n",
      "        - The momentum method\n",
      "        - Adaptive learning rates for each connection\n",
      "        - Rmsprop: Divide the gradient by a running average of its recent magnitude\n",
      "    7. Recurrent Neural Networks\n",
      "        - Modeling sequences: A brief overview\n",
      "        - Training RNNs with back propagation\n",
      "        - A toy example of training an RNN\n",
      "        - Why it is difficult to train an RNN\n",
      "        - Long-term Short-term-memory\n",
      "    8. Bayesian approaches, generalization and regularization\n",
      "        - Overview of ways to improve generalization\n",
      "        - Limiting the size of the weights\n",
      "        - Using noise as a regularizer\n",
      "        - Introduction to the full Bayesian approach\n",
      "        - The Bayesian interpretation of weight decay\n",
      "    9. Bayesian learning and dropout\n",
      "        - Why it helps to combine models\n",
      "        - Mixtures of Experts\n",
      "        - The idea of full Bayesian learning\n",
      "        - Making full Bayesian learning practical\n",
      "        - Dropout\n",
      "    11. Hopfield Nets\n",
      "        - Dealing with spurious minima\n",
      "        - Hopfield nets with hidden units\n",
      "        - Using stochastic units to improve search\n",
      "        - How a Boltzmann machine models data\n",
      "    12. Boltzmann machine learning\n",
      "        - Restricted Boltzmann Machines\n",
      "        - An example of RBM learning\n",
      "    13. Belief Nets\n",
      "        - The ups and downs of backpropagation\n",
      "        - Learning sigmoid belief nets\n",
      "        - The wake-sleep algorithm\n",
      "    14. Autoencoders\n",
      "        - From PCA to autoencoders\n",
      "        - Deep autoencoders\n",
      "        - Deep autoencoders for document retrieval\n",
      "        - Semantic hashing\n",
      "        - Learning binary codes for image retrieval\n",
      "        - Shallow autoencoders for pre-training"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- **Graduate Machine Learing** [page](http://www.cs.toronto.edu/~hinton/csc2515/lectures.html)\n",
      "    1. Overview of Machine Learning\n",
      "    2. Linear Regression\n",
      "    3. Linear Classification\n",
      "    4. Neural Networks trained by Backpropagation\n",
      "    5. Clustering and Mixture Models\n",
      "    6. Decision Trees and Mixtures of Experts\n",
      "    7. Continuous Latent Variable Models\n",
      "    8. Deep Belief Nets\n",
      "    9. Time-series Models\n",
      "    10. Nearest Neighbors and Kernel Density\n",
      "    11. Support Vector Machines\n",
      "    12. Applications of machine learning to language modeling and to retrieval of documents and images\n",
      "    13. Boosting and Naive Bayes\n",
      "    14. Gaussian Processes\n",
      "\n",
      "- **Advanced Graduate Machine Learning** [page](http://www.cs.toronto.edu/~hinton/csc2535/index.html)\n",
      "    1. Overview of Machine Learning and Graphical Models\n",
      "    2. Inference in Factor Graphs\n",
      "    3. Variational Inference and the wake-sleep algorithm\n",
      "    4. The origin of variational Bayes\n",
      "    5. Approximate Learning Methods for Energy-Based Models\n",
      "    6. Restricted Boltzmann machines\n",
      "    7. Deep Boltzmann machines\n",
      "    8. Object Recognition in Deep Neural Nets\n",
      "    9. Models of words and documents\n",
      "    10. Learning three-way interactions\n",
      "    11. Autoencoders for Image retrieval\n",
      "    12. Collaborative filtering and Missig Data Problems\n",
      "    13. Recurrent neural networks\n",
      "    14. Non-linear Dimensionality Reduction\n",
      "\n",
      "- **Unsupervised Feature Learning and Deep Learning** [page](http://deeplearning.stanford.edu/wiki/index.php/UFLDL_Tutorial)\n",
      "    1. Sparse Autoencoder\n",
      "        - Neural Networks\n",
      "        - Backpropagation Algorithm\n",
      "        - Gradient checking and advanced optimization\n",
      "        - Autoencoders and Sparsity\n",
      "        - Visualizing Trained Autoencoder\n",
      "        - Sparse Autoencoder Notation Summary\n",
      "    2. Vectorized implementation\n",
      "        - Vectorization\n",
      "        - Logistic Regression vectorization Example\n",
      "        - Neural Network Vectorization\n",
      "    3. Preprocessing: PCA and Whitening\n",
      "        - PCA\n",
      "        - Whitening\n",
      "        - Implementing PCA/Whitening\n",
      "    4. Softmax Regression\n",
      "        - Softmax Regression\n",
      "    5. Self-Taught Learning and Unsupervised Feature Learning\n",
      "        - Self-Taught Learning\n",
      "    6. Building Deep Networks for Classification\n",
      "        - From Self-Taught Learning to Deep Networks\n",
      "        - Deep Networks: Overview\n",
      "        - Stacked Autoencoders\n",
      "        - Fine-tuning Stacked Autoencoders\n",
      "    7. Linear Decoders with Autoencoders\n",
      "        - Linear Decoders\n",
      "    8. Working with Large Images\n",
      "        - Feature extraction using convolution\n",
      "        - Pooling\n",
      "        \n",
      "- **Reservoir computing** \n",
      "    1. Echo state network [matlab code](http://p-apex-g.jimdo.com/matlab-program/)\n",
      "        - Jaeger and Haas, Harnessing Nonlinearity: Predicting Chaotic Systems and Saving Energy in Wireless Communication, 2004, [page](http://courses.cs.tamu.edu/rgutier/cpsc636_s10/jaeger2004echoStateNetworks.pdf)\n",
      "        - Lukosevicius, Jaeger, Resevoir Computing Approaches to Recurrent Neural Network Training, [paper](http://web.info.uvt.ro/~dzaharie/cne2013/proiecte/tehnici/ReservoirComputing/ReservoirComputingApproaches.pdf)\n",
      "        - Verstraeten, et al, 2007, An Experimental unification of reservoir computing methods [paper](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.68.1435&rep=rep1&type=pdf)\n",
      "        - Buonomano, Maass, State-dependent computations: spatiotemporal processing in cortical networks [paper](http://www.silvalab.com/LMcourse/BuonoNRN09.pdf)\n",
      "        - Schrauwen, et al, An overview of reservoir computing: theory, applications, and implementations [paper](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.155.2814&rep=rep1&type=pdf)\n",
      "    2. Liquid state machine [wiki](http://en.wikipedia.org/wiki/Liquid_state_machine) [Wolfgang Maass](http://www.igi.tugraz.at/maass/)\n",
      "        - Maass, Wolfgang; Markram, Henry (2004), \"On the Computational Power of Recurrent Circuits of Spiking Neurons\", Journal of Computer and System Sciences 69 (4): 593\u2013616, doi:10.1016/j.jcss.2004.04.001 [paper](http://www.igi.tugraz.at/tnatschl/psfiles/135.pdf)\n",
      "        - Maass, Wolfgang; Natschl\u00e4ger, Thomas; Markram, Henry (November 2002), \"Real-time computing without stable states: a new framework for neural computation based on perturbations\", Neural Comput 14 (11): 2531\u201360, doi:10.1162/089976602760407955, PMID 12433288. [paper](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.131.277&rep=rep1&type=pdf)\n",
      "        - Wolfgang Maass, Thomas Natschl\u00e4ger, Henry Markram (2004), \"Computational Models for Generic Cortical Microcircuits\", In Computational Neuroscience: a Comprehensive Approach, Ch 18 18: 575\u2013605\n",
      "        - Fernando, Chrisantha; Sojakka, Sampsa; Of Series Lecture Notes In Computer Science, ISBN (2005), \"Pattern Recognition in a Bucket\", In Advances in Artificial Life: 978\u20133\n",
      "    3. Optoelectronic Reservoir Computing\n",
      "        - [paper](http://www.nature.com/srep/2012/120227/srep00287/full/srep00287.html)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- **Learn Data Science** [page](http://learnds.com)\n",
      "    1. Before you begin\n",
      "    2. Linear Regression - Overview\n",
      "    3. Linear Regression - Data Exploration - Lending Club\n",
      "    4. Linear Regression - Analysis\n",
      "    5. Logisitic Regression - Overview\n",
      "    6. Odds, LogOdds and Logit Function\n",
      "    7. Logistic Regression - Data Exploration\n",
      "    8. Logistic Regression - Analysis\n",
      "    9. Random Forests - Overview\n",
      "    10. Random Forests - Data Exploration\n",
      "    11. Random Forests - Analysis\n",
      "    12. K-Means Clustering - Overview\n",
      "    13. K-Means Clustering - Data Exploration\n",
      "    14. K-Means Clustering - Analysis\n",
      "    15. Linear Regression Overview worksheet\n",
      "    16. Linear Regression - Data Exploration - Lending Club Worksheet\n",
      "    17. Linear Regression - Analysis - Worksheet\n",
      "    18. Logistic Regression - Analysis - Worksheet\n",
      "    19. Random Forests - Analysis - Worksheet\n",
      "    20. Random Forests - Data Cleanup Worksheep\n",
      "    21. K-Means Clustering - Data Exploration - Worksheet\n",
      "    22. K-Means Clusering Analysis - Worksheet\n",
      "    23. A quick tour of the iPython notebook\n",
      "\n",
      "- **Large Scale Machine Learning** [page](http://www.cs.toronto.edu/~rsalakhu/STA4273_2015/)\n",
      "    1. Machine Learning: Introduction, Linear Models for Regression\n",
      "    2. Bayesian Framework: Bayesian Linear regression, Evidence Maximization, Linear Models of Classification\n",
      "    3. Classification\n",
      "    4. Graphical Models: Bayesian Networks, Markov Random Fields\n",
      "    5. Mixture Models an EM: Mixture of Gaussians, Generalized EM, Variational Bound\n",
      "    6. Variational Inference\n",
      "    7. Sampling Methods"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- **Probabilistic Graphical Models** [page](http://www.cs.toronto.edu/~zemel/inquiry/courses_home.php)\n",
      "    1. Introduction to Inference and learning\n",
      "    2. Simple classifiers\n",
      "    3. Undirected graphical models\n",
      "    4. Directed graphical models\n",
      "    5. Exact inference\n",
      "    6. Variational inference\n",
      "    7. Sampling\n",
      "    8. State space models\n",
      "    9. Conditional random fields\n",
      "    10. Gaussian Processes\n",
      "    11. Boltzmann Machines\n",
      "    \n",
      "- **Machine Learning and Data Mining** [page](http://www.cs.toronto.edu/~zemel/inquiry/courses_home.php?ID=4&SEM=7)\n",
      "    1. Introduction\n",
      "    2. Linear Regression\n",
      "    3. Linear Classifiers\n",
      "    4. Logistic Regression\n",
      "    5. Nonparametric Methods\n",
      "    6. Decision Trees\n",
      "    7. Multi-class Classifiers\n",
      "    8. Probabilistic Classifiers\n",
      "    9. Probabilistic Classifiers II\n",
      "    10. Neural Networks\n",
      "    11. Neural Networks II\n",
      "    12. Clustering\n",
      "    13. Mixtures of Gaussians and EM\n",
      "    14. Principal Components Analysis\n",
      "    15. Kernels & Margins\n",
      "    16. Support Vector Machines\n",
      "    17. Ensemble methods\n",
      "    18. Ensemble Methods II\n",
      "    19. Bayesian Methods\n",
      "    20. Reinforcement Learning\n",
      "\n",
      "- **Statistical Models of Networks, Graphs, and other Relational Structures** [page](http://danroy.org/teaching/2014/STA4513/)\n",
      "\n",
      "    1. Latent Variable Models\n",
      "    2. Exponential Random Graph Models\n",
      "    3. Exchangeability of Sequences and Graphs\n",
      "    4. Graphons\n",
      "    5. Graphings and unimodeular random networks\n",
      "    6. Graphings and unimodeular random networks\n",
      "\n",
      "- **Multivariate Analysis** [page](http://www.utstat.toronto.edu/~rsalakhu/StaD37H/)\n",
      "    1. Organization of Data; Matrix Algebra\n",
      "    2. Random vectors; random sampling\n",
      "    3. Multivariate normal distributions\n",
      "    4. Inferences about the Mean vector\n",
      "    5. Comparisons of several multivariate means\n",
      "    6. Comparisons of Several Multivariate Means\n",
      "    7. Principal Component Analysis\n",
      "    8. Factor Analysis\n",
      "    9. Discrimination and Classification\n",
      "    10. Canonical Correlation Analysis\n",
      "    11. Multivariate Linear Regression"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}